{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bibliotecas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import plotly.express as px\r\n",
    "\r\n",
    "from sklearn import datasets\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\r\n",
    "\r\n",
    "from keras.models import Sequential, load_model\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Dropout\r\n",
    "from keras.utils.vis_utils import plot_model\r\n",
    "from keras.utils.np_utils import  to_categorical\r\n",
    "from keras.callbacks import ModelCheckpoint\r\n",
    "\r\n",
    "\r\n",
    "# Ignorar Avisos desnecessários\r\n",
    "import warnings\r\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\r\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Confere se temos GPU instalada"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\r\n",
    "#print(tf.config.experimental.list_physical_devices())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funções Auxiliares"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def plot_history(history):\r\n",
    "    acc = history.history['accuracy']\r\n",
    "    val_acc = history.history['val_accuracy']\r\n",
    "    loss = history.history['loss']\r\n",
    "    val_loss = history.history['val_loss']\r\n",
    "    x = range(1, len(acc) + 1)\r\n",
    "\r\n",
    "    plt.figure(figsize=(12, 5))\r\n",
    "    plt.subplot(1, 2, 1)\r\n",
    "    plt.plot(x, acc, 'b', label='Training Accuracy')\r\n",
    "    plt.plot(x, val_acc, 'r', label='Validation Accuracy')\r\n",
    "    plt.title('Acuracidade do Treino e Validação')\r\n",
    "    plt.legend()\r\n",
    "    plt.subplot(1, 2, 2)\r\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\r\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\r\n",
    "    plt.title('Pèrda do Treino e Validação')\r\n",
    "    plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Análise dos Dados\r\n",
    "\r\n",
    "## Carregando a base de dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "iris = datasets.load_iris()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distribuição dos dados em relação a variável resposta. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df = px.data.iris()\r\n",
    "fig = px.scatter_matrix(df, dimensions=[\"sepal_width\", \"sepal_length\", \"petal_width\", \"petal_length\"], color=\"species\")\r\n",
    "fig.update_traces(diagonal_visible=False, showupperhalf=False)\r\n",
    "fig.update_layout(title='Iris Data set', width=1000, height=600)\r\n",
    "fig.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "diagonal": {
          "visible": false
         },
         "dimensions": [
          {
           "axis": {
            "matches": true
           },
           "label": "sepal_width",
           "values": [
            3.5,
            3,
            3.2,
            3.1,
            3.6,
            3.9,
            3.4,
            3.4,
            2.9,
            3.1,
            3.7,
            3.4,
            3,
            3,
            4,
            4.4,
            3.9,
            3.5,
            3.8,
            3.8,
            3.4,
            3.7,
            3.6,
            3.3,
            3.4,
            3,
            3.4,
            3.5,
            3.4,
            3.2,
            3.1,
            3.4,
            4.1,
            4.2,
            3.1,
            3.2,
            3.5,
            3.1,
            3,
            3.4,
            3.5,
            2.3,
            3.2,
            3.5,
            3.8,
            3,
            3.8,
            3.2,
            3.7,
            3.3
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "sepal_length",
           "values": [
            5.1,
            4.9,
            4.7,
            4.6,
            5,
            5.4,
            4.6,
            5,
            4.4,
            4.9,
            5.4,
            4.8,
            4.8,
            4.3,
            5.8,
            5.7,
            5.4,
            5.1,
            5.7,
            5.1,
            5.4,
            5.1,
            4.6,
            5.1,
            4.8,
            5,
            5,
            5.2,
            5.2,
            4.7,
            4.8,
            5.4,
            5.2,
            5.5,
            4.9,
            5,
            5.5,
            4.9,
            4.4,
            5.1,
            5,
            4.5,
            4.4,
            5,
            5.1,
            4.8,
            5.1,
            4.6,
            5.3,
            5
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "petal_width",
           "values": [
            0.2,
            0.2,
            0.2,
            0.2,
            0.2,
            0.4,
            0.3,
            0.2,
            0.2,
            0.1,
            0.2,
            0.2,
            0.1,
            0.1,
            0.2,
            0.4,
            0.4,
            0.3,
            0.3,
            0.3,
            0.2,
            0.4,
            0.2,
            0.5,
            0.2,
            0.2,
            0.4,
            0.2,
            0.2,
            0.2,
            0.2,
            0.4,
            0.1,
            0.2,
            0.1,
            0.2,
            0.2,
            0.1,
            0.2,
            0.2,
            0.3,
            0.3,
            0.2,
            0.6,
            0.4,
            0.3,
            0.2,
            0.2,
            0.2,
            0.2
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "petal_length",
           "values": [
            1.4,
            1.4,
            1.3,
            1.5,
            1.4,
            1.7,
            1.4,
            1.5,
            1.4,
            1.5,
            1.5,
            1.6,
            1.4,
            1.1,
            1.2,
            1.5,
            1.3,
            1.4,
            1.7,
            1.5,
            1.7,
            1.5,
            1,
            1.7,
            1.9,
            1.6,
            1.6,
            1.5,
            1.4,
            1.6,
            1.6,
            1.5,
            1.5,
            1.4,
            1.5,
            1.2,
            1.3,
            1.5,
            1.3,
            1.5,
            1.3,
            1.3,
            1.3,
            1.6,
            1.9,
            1.4,
            1.6,
            1.4,
            1.5,
            1.4
           ]
          }
         ],
         "hovertemplate": "species=setosa<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>",
         "legendgroup": "setosa",
         "marker": {
          "color": "#636efa",
          "symbol": "circle"
         },
         "name": "setosa",
         "showlegend": true,
         "showupperhalf": false,
         "type": "splom"
        },
        {
         "diagonal": {
          "visible": false
         },
         "dimensions": [
          {
           "axis": {
            "matches": true
           },
           "label": "sepal_width",
           "values": [
            3.2,
            3.2,
            3.1,
            2.3,
            2.8,
            2.8,
            3.3,
            2.4,
            2.9,
            2.7,
            2,
            3,
            2.2,
            2.9,
            2.9,
            3.1,
            3,
            2.7,
            2.2,
            2.5,
            3.2,
            2.8,
            2.5,
            2.8,
            2.9,
            3,
            2.8,
            3,
            2.9,
            2.6,
            2.4,
            2.4,
            2.7,
            2.7,
            3,
            3.4,
            3.1,
            2.3,
            3,
            2.5,
            2.6,
            3,
            2.6,
            2.3,
            2.7,
            3,
            2.9,
            2.9,
            2.5,
            2.8
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "sepal_length",
           "values": [
            7,
            6.4,
            6.9,
            5.5,
            6.5,
            5.7,
            6.3,
            4.9,
            6.6,
            5.2,
            5,
            5.9,
            6,
            6.1,
            5.6,
            6.7,
            5.6,
            5.8,
            6.2,
            5.6,
            5.9,
            6.1,
            6.3,
            6.1,
            6.4,
            6.6,
            6.8,
            6.7,
            6,
            5.7,
            5.5,
            5.5,
            5.8,
            6,
            5.4,
            6,
            6.7,
            6.3,
            5.6,
            5.5,
            5.5,
            6.1,
            5.8,
            5,
            5.6,
            5.7,
            5.7,
            6.2,
            5.1,
            5.7
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "petal_width",
           "values": [
            1.4,
            1.5,
            1.5,
            1.3,
            1.5,
            1.3,
            1.6,
            1,
            1.3,
            1.4,
            1,
            1.5,
            1,
            1.4,
            1.3,
            1.4,
            1.5,
            1,
            1.5,
            1.1,
            1.8,
            1.3,
            1.5,
            1.2,
            1.3,
            1.4,
            1.4,
            1.7,
            1.5,
            1,
            1.1,
            1,
            1.2,
            1.6,
            1.5,
            1.6,
            1.5,
            1.3,
            1.3,
            1.3,
            1.2,
            1.4,
            1.2,
            1,
            1.3,
            1.2,
            1.3,
            1.3,
            1.1,
            1.3
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "petal_length",
           "values": [
            4.7,
            4.5,
            4.9,
            4,
            4.6,
            4.5,
            4.7,
            3.3,
            4.6,
            3.9,
            3.5,
            4.2,
            4,
            4.7,
            3.6,
            4.4,
            4.5,
            4.1,
            4.5,
            3.9,
            4.8,
            4,
            4.9,
            4.7,
            4.3,
            4.4,
            4.8,
            5,
            4.5,
            3.5,
            3.8,
            3.7,
            3.9,
            5.1,
            4.5,
            4.5,
            4.7,
            4.4,
            4.1,
            4,
            4.4,
            4.6,
            4,
            3.3,
            4.2,
            4.2,
            4.2,
            4.3,
            3,
            4.1
           ]
          }
         ],
         "hovertemplate": "species=versicolor<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>",
         "legendgroup": "versicolor",
         "marker": {
          "color": "#EF553B",
          "symbol": "circle"
         },
         "name": "versicolor",
         "showlegend": true,
         "showupperhalf": false,
         "type": "splom"
        },
        {
         "diagonal": {
          "visible": false
         },
         "dimensions": [
          {
           "axis": {
            "matches": true
           },
           "label": "sepal_width",
           "values": [
            3.3,
            2.7,
            3,
            2.9,
            3,
            3,
            2.5,
            2.9,
            2.5,
            3.6,
            3.2,
            2.7,
            3,
            2.5,
            2.8,
            3.2,
            3,
            3.8,
            2.6,
            2.2,
            3.2,
            2.8,
            2.8,
            2.7,
            3.3,
            3.2,
            2.8,
            3,
            2.8,
            3,
            2.8,
            3.8,
            2.8,
            2.8,
            2.6,
            3,
            3.4,
            3.1,
            3,
            3.1,
            3.1,
            3.1,
            2.7,
            3.2,
            3.3,
            3,
            2.5,
            3,
            3.4,
            3
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "sepal_length",
           "values": [
            6.3,
            5.8,
            7.1,
            6.3,
            6.5,
            7.6,
            4.9,
            7.3,
            6.7,
            7.2,
            6.5,
            6.4,
            6.8,
            5.7,
            5.8,
            6.4,
            6.5,
            7.7,
            7.7,
            6,
            6.9,
            5.6,
            7.7,
            6.3,
            6.7,
            7.2,
            6.2,
            6.1,
            6.4,
            7.2,
            7.4,
            7.9,
            6.4,
            6.3,
            6.1,
            7.7,
            6.3,
            6.4,
            6,
            6.9,
            6.7,
            6.9,
            5.8,
            6.8,
            6.7,
            6.7,
            6.3,
            6.5,
            6.2,
            5.9
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "petal_width",
           "values": [
            2.5,
            1.9,
            2.1,
            1.8,
            2.2,
            2.1,
            1.7,
            1.8,
            1.8,
            2.5,
            2,
            1.9,
            2.1,
            2,
            2.4,
            2.3,
            1.8,
            2.2,
            2.3,
            1.5,
            2.3,
            2,
            2,
            1.8,
            2.1,
            1.8,
            1.8,
            1.8,
            2.1,
            1.6,
            1.9,
            2,
            2.2,
            1.5,
            1.4,
            2.3,
            2.4,
            1.8,
            1.8,
            2.1,
            2.4,
            2.3,
            1.9,
            2.3,
            2.5,
            2.3,
            1.9,
            2,
            2.3,
            1.8
           ]
          },
          {
           "axis": {
            "matches": true
           },
           "label": "petal_length",
           "values": [
            6,
            5.1,
            5.9,
            5.6,
            5.8,
            6.6,
            4.5,
            6.3,
            5.8,
            6.1,
            5.1,
            5.3,
            5.5,
            5,
            5.1,
            5.3,
            5.5,
            6.7,
            6.9,
            5,
            5.7,
            4.9,
            6.7,
            4.9,
            5.7,
            6,
            4.8,
            4.9,
            5.6,
            5.8,
            6.1,
            6.4,
            5.6,
            5.1,
            5.6,
            6.1,
            5.6,
            5.5,
            4.8,
            5.4,
            5.6,
            5.1,
            5.1,
            5.9,
            5.7,
            5.2,
            5,
            5.2,
            5.4,
            5.1
           ]
          }
         ],
         "hovertemplate": "species=virginica<br>%{xaxis.title.text}=%{x}<br>%{yaxis.title.text}=%{y}<extra></extra>",
         "legendgroup": "virginica",
         "marker": {
          "color": "#00cc96",
          "symbol": "circle"
         },
         "name": "virginica",
         "showlegend": true,
         "showupperhalf": false,
         "type": "splom"
        }
       ],
       "layout": {
        "dragmode": "select",
        "height": 600,
        "legend": {
         "title": {
          "text": "species"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Iris Data set"
        },
        "width": 1000
       }
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Criando os dados para o treino."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "features = iris.feature_names\r\n",
    "X = iris.data\r\n",
    "X[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "classes = iris.target_names\r\n",
    "y = iris.target\r\n",
    "y[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Temos uma distribuição uniforme dos 3 itens da variável resposta"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sns.countplot(y)\r\n",
    "plt.title('Distribuição da variável Target')\r\n",
    "plt.xlabel('Target')\r\n",
    "plt.xticks(ticks=range(0,3), labels=classes)\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padronizando os dados com o MinMaxScaler do SKlearn. \r\n",
    "### Dados padronizados entre 0 e 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "scaler = MinMaxScaler()\r\n",
    "X = scaler.fit_transform(X)\r\n",
    "X[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
       "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
       "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
       "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
       "       [0.19444444, 0.66666667, 0.06779661, 0.04166667]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformando a variável resposta em \"One-hot vector\" para utilizá-la no modelo de MLP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "num_features = X.shape[1]\r\n",
    "y = to_categorical(y)\r\n",
    "num_classes = y.shape[1]\r\n",
    "print(y.shape)\r\n",
    "y[:5]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(150, 3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dividindo a Base de Dados entre Treino e Teste, com 20% para os testes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "y_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(120, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelo de MLP\r\n",
    "\r\n",
    "## Criando o modelo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Dense(8, input_dim=num_features, activation='relu'))\r\n",
    "model.add(Dense(num_classes , activation='softmax'))\r\n",
    "print(model.summary())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compilando e treinando o modelo. \r\n",
    "### Vamos utilizar a função de Callback ModelCheckPointer para salvar o modelo com a melhor accuracia na base de validação (que é a mesma de teste final pois temos uma base muito pequena)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "%%time\r\n",
    "checkpointer = ModelCheckpoint(filepath='./modelo_mlp_ex3_1.hdf5', verbose=1,  save_best_only=True, monitor='val_accuracy')\r\n",
    "\r\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, batch_size=12, callbacks=[checkpointer])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 1.1287 - accuracy: 0.2500\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.36667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 18ms/step - loss: 1.0447 - accuracy: 0.3250 - val_loss: 0.9874 - val_accuracy: 0.3667\n",
      "Epoch 2/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 1.2212 - accuracy: 0.0833\n",
      "Epoch 00002: val_accuracy did not improve from 0.36667\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.0224 - accuracy: 0.3250 - val_loss: 0.9690 - val_accuracy: 0.3667\n",
      "Epoch 3/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7429 - accuracy: 0.6667\n",
      "Epoch 00003: val_accuracy did not improve from 0.36667\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.0019 - accuracy: 0.3250 - val_loss: 0.9521 - val_accuracy: 0.3667\n",
      "Epoch 4/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 1.0030 - accuracy: 0.3333\n",
      "Epoch 00004: val_accuracy did not improve from 0.36667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.9846 - accuracy: 0.3250 - val_loss: 0.9355 - val_accuracy: 0.3667\n",
      "Epoch 5/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 1.0605 - accuracy: 0.1667\n",
      "Epoch 00005: val_accuracy did not improve from 0.36667\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.9672 - accuracy: 0.3333 - val_loss: 0.9204 - val_accuracy: 0.3667\n",
      "Epoch 6/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8899 - accuracy: 0.4167\n",
      "Epoch 00006: val_accuracy improved from 0.36667 to 0.40000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.9511 - accuracy: 0.3417 - val_loss: 0.9057 - val_accuracy: 0.4000\n",
      "Epoch 7/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 1.0072 - accuracy: 0.2500\n",
      "Epoch 00007: val_accuracy improved from 0.40000 to 0.46667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.9353 - accuracy: 0.3500 - val_loss: 0.8928 - val_accuracy: 0.4667\n",
      "Epoch 8/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8923 - accuracy: 0.5000\n",
      "Epoch 00008: val_accuracy improved from 0.46667 to 0.56667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.9206 - accuracy: 0.4500 - val_loss: 0.8803 - val_accuracy: 0.5667\n",
      "Epoch 9/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.9923 - accuracy: 0.3333\n",
      "Epoch 00009: val_accuracy improved from 0.56667 to 0.63333, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.9082 - accuracy: 0.5167 - val_loss: 0.8682 - val_accuracy: 0.6333\n",
      "Epoch 10/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8609 - accuracy: 0.6667\n",
      "Epoch 00010: val_accuracy improved from 0.63333 to 0.66667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.8940 - accuracy: 0.6167 - val_loss: 0.8568 - val_accuracy: 0.6667\n",
      "Epoch 11/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.9041 - accuracy: 0.5000\n",
      "Epoch 00011: val_accuracy did not improve from 0.66667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8818 - accuracy: 0.6583 - val_loss: 0.8468 - val_accuracy: 0.6667\n",
      "Epoch 12/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8940 - accuracy: 0.7500\n",
      "Epoch 00012: val_accuracy improved from 0.66667 to 0.70000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.8701 - accuracy: 0.6583 - val_loss: 0.8366 - val_accuracy: 0.7000\n",
      "Epoch 13/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7936 - accuracy: 0.8333\n",
      "Epoch 00013: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8578 - accuracy: 0.6583 - val_loss: 0.8266 - val_accuracy: 0.7000\n",
      "Epoch 14/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.9235 - accuracy: 0.5833\n",
      "Epoch 00014: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8475 - accuracy: 0.6583 - val_loss: 0.8165 - val_accuracy: 0.7000\n",
      "Epoch 15/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8245 - accuracy: 0.8333\n",
      "Epoch 00015: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8362 - accuracy: 0.6583 - val_loss: 0.8073 - val_accuracy: 0.7000\n",
      "Epoch 16/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8106 - accuracy: 0.6667\n",
      "Epoch 00016: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8261 - accuracy: 0.6583 - val_loss: 0.7982 - val_accuracy: 0.7000\n",
      "Epoch 17/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8265 - accuracy: 0.6667\n",
      "Epoch 00017: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8164 - accuracy: 0.6583 - val_loss: 0.7890 - val_accuracy: 0.7000\n",
      "Epoch 18/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7961 - accuracy: 0.5833\n",
      "Epoch 00018: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.8064 - accuracy: 0.6583 - val_loss: 0.7799 - val_accuracy: 0.7000\n",
      "Epoch 19/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8331 - accuracy: 0.5833\n",
      "Epoch 00019: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7967 - accuracy: 0.6667 - val_loss: 0.7712 - val_accuracy: 0.7000\n",
      "Epoch 20/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7372 - accuracy: 0.9167\n",
      "Epoch 00020: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7874 - accuracy: 0.6750 - val_loss: 0.7623 - val_accuracy: 0.7000\n",
      "Epoch 21/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7932 - accuracy: 0.5833\n",
      "Epoch 00021: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7780 - accuracy: 0.6833 - val_loss: 0.7534 - val_accuracy: 0.7000\n",
      "Epoch 22/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7966 - accuracy: 0.6667\n",
      "Epoch 00022: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7690 - accuracy: 0.6833 - val_loss: 0.7446 - val_accuracy: 0.7000\n",
      "Epoch 23/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7247 - accuracy: 0.6667\n",
      "Epoch 00023: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7598 - accuracy: 0.6833 - val_loss: 0.7362 - val_accuracy: 0.7000\n",
      "Epoch 24/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7572 - accuracy: 0.6667\n",
      "Epoch 00024: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7513 - accuracy: 0.6833 - val_loss: 0.7279 - val_accuracy: 0.7000\n",
      "Epoch 25/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8379 - accuracy: 0.4167\n",
      "Epoch 00025: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7423 - accuracy: 0.6917 - val_loss: 0.7201 - val_accuracy: 0.7000\n",
      "Epoch 26/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7669 - accuracy: 0.5833\n",
      "Epoch 00026: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7339 - accuracy: 0.6917 - val_loss: 0.7118 - val_accuracy: 0.7000\n",
      "Epoch 27/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.8125 - accuracy: 0.5000\n",
      "Epoch 00027: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7257 - accuracy: 0.6917 - val_loss: 0.7038 - val_accuracy: 0.7000\n",
      "Epoch 28/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6309 - accuracy: 0.9167\n",
      "Epoch 00028: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7174 - accuracy: 0.6917 - val_loss: 0.6959 - val_accuracy: 0.7000\n",
      "Epoch 29/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6867 - accuracy: 0.6667\n",
      "Epoch 00029: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7097 - accuracy: 0.6917 - val_loss: 0.6882 - val_accuracy: 0.7000\n",
      "Epoch 30/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6279 - accuracy: 0.8333\n",
      "Epoch 00030: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.7015 - accuracy: 0.6917 - val_loss: 0.6807 - val_accuracy: 0.7000\n",
      "Epoch 31/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6042 - accuracy: 0.8333\n",
      "Epoch 00031: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6939 - accuracy: 0.6917 - val_loss: 0.6732 - val_accuracy: 0.7000\n",
      "Epoch 32/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7885 - accuracy: 0.4167\n",
      "Epoch 00032: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.6917 - val_loss: 0.6664 - val_accuracy: 0.7000\n",
      "Epoch 33/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6862 - accuracy: 0.6667\n",
      "Epoch 00033: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6790 - accuracy: 0.6917 - val_loss: 0.6592 - val_accuracy: 0.7000\n",
      "Epoch 34/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7385 - accuracy: 0.5000\n",
      "Epoch 00034: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.6917 - val_loss: 0.6521 - val_accuracy: 0.7000\n",
      "Epoch 35/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7133 - accuracy: 0.5833\n",
      "Epoch 00035: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6649 - accuracy: 0.6917 - val_loss: 0.6452 - val_accuracy: 0.7000\n",
      "Epoch 36/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6639 - accuracy: 0.6667\n",
      "Epoch 00036: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6579 - accuracy: 0.6917 - val_loss: 0.6386 - val_accuracy: 0.7000\n",
      "Epoch 37/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5140 - accuracy: 0.9167\n",
      "Epoch 00037: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.6917 - val_loss: 0.6317 - val_accuracy: 0.7000\n",
      "Epoch 38/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5266 - accuracy: 0.8333\n",
      "Epoch 00038: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6448 - accuracy: 0.6917 - val_loss: 0.6252 - val_accuracy: 0.7000\n",
      "Epoch 39/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6568 - accuracy: 0.6667\n",
      "Epoch 00039: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6382 - accuracy: 0.6917 - val_loss: 0.6192 - val_accuracy: 0.7000\n",
      "Epoch 40/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5375 - accuracy: 0.7500\n",
      "Epoch 00040: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6323 - accuracy: 0.6833 - val_loss: 0.6128 - val_accuracy: 0.7000\n",
      "Epoch 41/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5471 - accuracy: 0.8333\n",
      "Epoch 00041: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6259 - accuracy: 0.6833 - val_loss: 0.6069 - val_accuracy: 0.7000\n",
      "Epoch 42/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4520 - accuracy: 1.0000\n",
      "Epoch 00042: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6201 - accuracy: 0.6917 - val_loss: 0.6009 - val_accuracy: 0.7000\n",
      "Epoch 43/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6201 - accuracy: 0.6667\n",
      "Epoch 00043: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6142 - accuracy: 0.6917 - val_loss: 0.5958 - val_accuracy: 0.7000\n",
      "Epoch 44/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6930 - accuracy: 0.5000\n",
      "Epoch 00044: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6086 - accuracy: 0.6917 - val_loss: 0.5901 - val_accuracy: 0.7000\n",
      "Epoch 45/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5525 - accuracy: 0.8333\n",
      "Epoch 00045: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.6031 - accuracy: 0.6917 - val_loss: 0.5847 - val_accuracy: 0.7000\n",
      "Epoch 46/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5405 - accuracy: 0.7500\n",
      "Epoch 00046: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5978 - accuracy: 0.7083 - val_loss: 0.5795 - val_accuracy: 0.7000\n",
      "Epoch 47/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6112 - accuracy: 0.7500\n",
      "Epoch 00047: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5925 - accuracy: 0.7083 - val_loss: 0.5745 - val_accuracy: 0.7000\n",
      "Epoch 48/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6340 - accuracy: 0.6667\n",
      "Epoch 00048: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5875 - accuracy: 0.7083 - val_loss: 0.5695 - val_accuracy: 0.7000\n",
      "Epoch 49/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3898 - accuracy: 1.0000\n",
      "Epoch 00049: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5831 - accuracy: 0.7083 - val_loss: 0.5640 - val_accuracy: 0.7000\n",
      "Epoch 50/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.7261 - accuracy: 0.6667\n",
      "Epoch 00050: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5785 - accuracy: 0.7083 - val_loss: 0.5604 - val_accuracy: 0.7000\n",
      "Epoch 51/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5467 - accuracy: 0.9167\n",
      "Epoch 00051: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5734 - accuracy: 0.7083 - val_loss: 0.5552 - val_accuracy: 0.7000\n",
      "Epoch 52/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6841 - accuracy: 0.5833\n",
      "Epoch 00052: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5687 - accuracy: 0.7167 - val_loss: 0.5507 - val_accuracy: 0.7000\n",
      "Epoch 53/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5426 - accuracy: 0.6667\n",
      "Epoch 00053: val_accuracy did not improve from 0.70000\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.5640 - accuracy: 0.7167 - val_loss: 0.5467 - val_accuracy: 0.7000\n",
      "Epoch 54/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5811 - accuracy: 0.5833\n",
      "Epoch 00054: val_accuracy improved from 0.70000 to 0.73333, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5597 - accuracy: 0.7333 - val_loss: 0.5427 - val_accuracy: 0.7333\n",
      "Epoch 55/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6006 - accuracy: 0.6667\n",
      "Epoch 00055: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5555 - accuracy: 0.7250 - val_loss: 0.5384 - val_accuracy: 0.7000\n",
      "Epoch 56/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4858 - accuracy: 0.8333\n",
      "Epoch 00056: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5514 - accuracy: 0.7250 - val_loss: 0.5342 - val_accuracy: 0.7333\n",
      "Epoch 57/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6179 - accuracy: 0.6667\n",
      "Epoch 00057: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.7250 - val_loss: 0.5305 - val_accuracy: 0.7333\n",
      "Epoch 58/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5352 - accuracy: 0.8333\n",
      "Epoch 00058: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5434 - accuracy: 0.7333 - val_loss: 0.5266 - val_accuracy: 0.7333\n",
      "Epoch 59/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5023 - accuracy: 0.7500\n",
      "Epoch 00059: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5398 - accuracy: 0.7333 - val_loss: 0.5229 - val_accuracy: 0.7333\n",
      "Epoch 60/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4604 - accuracy: 0.8333\n",
      "Epoch 00060: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5359 - accuracy: 0.7333 - val_loss: 0.5192 - val_accuracy: 0.7333\n",
      "Epoch 61/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6441 - accuracy: 0.7500\n",
      "Epoch 00061: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.7333 - val_loss: 0.5155 - val_accuracy: 0.7333\n",
      "Epoch 62/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5187 - accuracy: 0.6667\n",
      "Epoch 00062: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5290 - accuracy: 0.7333 - val_loss: 0.5121 - val_accuracy: 0.7333\n",
      "Epoch 63/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4559 - accuracy: 0.8333\n",
      "Epoch 00063: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5255 - accuracy: 0.7333 - val_loss: 0.5084 - val_accuracy: 0.7333\n",
      "Epoch 64/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5538 - accuracy: 0.7500\n",
      "Epoch 00064: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5224 - accuracy: 0.7333 - val_loss: 0.5055 - val_accuracy: 0.7333\n",
      "Epoch 65/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4767 - accuracy: 0.7500\n",
      "Epoch 00065: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7333 - val_loss: 0.5024 - val_accuracy: 0.7333\n",
      "Epoch 66/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5254 - accuracy: 0.5833\n",
      "Epoch 00066: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5158 - accuracy: 0.7333 - val_loss: 0.4990 - val_accuracy: 0.7333\n",
      "Epoch 67/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4633 - accuracy: 0.7500\n",
      "Epoch 00067: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5128 - accuracy: 0.7333 - val_loss: 0.4957 - val_accuracy: 0.7333\n",
      "Epoch 68/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4603 - accuracy: 0.8333\n",
      "Epoch 00068: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5098 - accuracy: 0.7333 - val_loss: 0.4925 - val_accuracy: 0.7333\n",
      "Epoch 69/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5024 - accuracy: 0.8333\n",
      "Epoch 00069: val_accuracy did not improve from 0.73333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.7333 - val_loss: 0.4895 - val_accuracy: 0.7333\n",
      "Epoch 70/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5017 - accuracy: 0.7500\n",
      "Epoch 00070: val_accuracy improved from 0.73333 to 0.76667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.5042 - accuracy: 0.7333 - val_loss: 0.4867 - val_accuracy: 0.7667\n",
      "Epoch 71/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5109 - accuracy: 0.8333\n",
      "Epoch 00071: val_accuracy did not improve from 0.76667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.5014 - accuracy: 0.7333 - val_loss: 0.4838 - val_accuracy: 0.7667\n",
      "Epoch 72/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5053 - accuracy: 0.6667\n",
      "Epoch 00072: val_accuracy did not improve from 0.76667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.7333 - val_loss: 0.4812 - val_accuracy: 0.7667\n",
      "Epoch 73/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5147 - accuracy: 0.8333\n",
      "Epoch 00073: val_accuracy improved from 0.76667 to 0.80000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4961 - accuracy: 0.7333 - val_loss: 0.4787 - val_accuracy: 0.8000\n",
      "Epoch 74/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5370 - accuracy: 0.7500\n",
      "Epoch 00074: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4936 - accuracy: 0.7417 - val_loss: 0.4762 - val_accuracy: 0.8000\n",
      "Epoch 75/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4902 - accuracy: 0.6667\n",
      "Epoch 00075: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4912 - accuracy: 0.7417 - val_loss: 0.4734 - val_accuracy: 0.8000\n",
      "Epoch 76/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4111 - accuracy: 0.8333\n",
      "Epoch 00076: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7583 - val_loss: 0.4711 - val_accuracy: 0.8000\n",
      "Epoch 77/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4864 - accuracy: 0.6667\n",
      "Epoch 00077: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4863 - accuracy: 0.7583 - val_loss: 0.4687 - val_accuracy: 0.8000\n",
      "Epoch 78/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.6146 - accuracy: 0.5833\n",
      "Epoch 00078: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.7667 - val_loss: 0.4663 - val_accuracy: 0.8000\n",
      "Epoch 79/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5498 - accuracy: 0.8333\n",
      "Epoch 00079: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7583 - val_loss: 0.4637 - val_accuracy: 0.8000\n",
      "Epoch 80/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4301 - accuracy: 0.8333\n",
      "Epoch 00080: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.7583 - val_loss: 0.4612 - val_accuracy: 0.8000\n",
      "Epoch 81/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4527 - accuracy: 0.7500\n",
      "Epoch 00081: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4773 - accuracy: 0.7667 - val_loss: 0.4591 - val_accuracy: 0.8000\n",
      "Epoch 82/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4636 - accuracy: 0.7500\n",
      "Epoch 00082: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4752 - accuracy: 0.7667 - val_loss: 0.4571 - val_accuracy: 0.8000\n",
      "Epoch 83/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4857 - accuracy: 0.7500\n",
      "Epoch 00083: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4731 - accuracy: 0.7833 - val_loss: 0.4550 - val_accuracy: 0.8000\n",
      "Epoch 84/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5210 - accuracy: 0.8333\n",
      "Epoch 00084: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4711 - accuracy: 0.7833 - val_loss: 0.4529 - val_accuracy: 0.8000\n",
      "Epoch 85/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4692 - accuracy: 0.9167\n",
      "Epoch 00085: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4692 - accuracy: 0.7833 - val_loss: 0.4506 - val_accuracy: 0.8000\n",
      "Epoch 86/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4290 - accuracy: 0.9167\n",
      "Epoch 00086: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4670 - accuracy: 0.7833 - val_loss: 0.4486 - val_accuracy: 0.8000\n",
      "Epoch 87/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4319 - accuracy: 0.9167\n",
      "Epoch 00087: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.7917 - val_loss: 0.4467 - val_accuracy: 0.8000\n",
      "Epoch 88/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4403 - accuracy: 1.0000\n",
      "Epoch 00088: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4633 - accuracy: 0.8000 - val_loss: 0.4446 - val_accuracy: 0.8000\n",
      "Epoch 89/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2756 - accuracy: 0.9167\n",
      "Epoch 00089: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4613 - accuracy: 0.8083 - val_loss: 0.4427 - val_accuracy: 0.8000\n",
      "Epoch 90/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4163 - accuracy: 0.8333\n",
      "Epoch 00090: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4595 - accuracy: 0.8167 - val_loss: 0.4409 - val_accuracy: 0.8000\n",
      "Epoch 91/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3976 - accuracy: 0.8333\n",
      "Epoch 00091: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.8083 - val_loss: 0.4387 - val_accuracy: 0.8000\n",
      "Epoch 92/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4893 - accuracy: 0.6667\n",
      "Epoch 00092: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.8167 - val_loss: 0.4371 - val_accuracy: 0.8000\n",
      "Epoch 93/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5593 - accuracy: 0.7500\n",
      "Epoch 00093: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4540 - accuracy: 0.8167 - val_loss: 0.4354 - val_accuracy: 0.8000\n",
      "Epoch 94/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4925 - accuracy: 0.7500\n",
      "Epoch 00094: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4523 - accuracy: 0.8167 - val_loss: 0.4336 - val_accuracy: 0.8000\n",
      "Epoch 95/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4143 - accuracy: 0.9167\n",
      "Epoch 00095: val_accuracy did not improve from 0.80000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4505 - accuracy: 0.8167 - val_loss: 0.4314 - val_accuracy: 0.8000\n",
      "Epoch 96/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5663 - accuracy: 0.7500\n",
      "Epoch 00096: val_accuracy improved from 0.80000 to 0.83333, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4488 - accuracy: 0.8167 - val_loss: 0.4298 - val_accuracy: 0.8333\n",
      "Epoch 97/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5060 - accuracy: 0.7500\n",
      "Epoch 00097: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8167 - val_loss: 0.4276 - val_accuracy: 0.8000\n",
      "Epoch 98/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4799 - accuracy: 0.7500\n",
      "Epoch 00098: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4450 - accuracy: 0.8167 - val_loss: 0.4259 - val_accuracy: 0.8333\n",
      "Epoch 99/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3414 - accuracy: 0.8333\n",
      "Epoch 00099: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4435 - accuracy: 0.8250 - val_loss: 0.4243 - val_accuracy: 0.8333\n",
      "Epoch 100/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3790 - accuracy: 0.9167\n",
      "Epoch 00100: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4417 - accuracy: 0.8333 - val_loss: 0.4225 - val_accuracy: 0.8333\n",
      "Epoch 101/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4324 - accuracy: 0.7500\n",
      "Epoch 00101: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4400 - accuracy: 0.8333 - val_loss: 0.4206 - val_accuracy: 0.8333\n",
      "Epoch 102/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5798 - accuracy: 0.8333\n",
      "Epoch 00102: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4383 - accuracy: 0.8333 - val_loss: 0.4189 - val_accuracy: 0.8333\n",
      "Epoch 103/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5288 - accuracy: 0.7500\n",
      "Epoch 00103: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4367 - accuracy: 0.8333 - val_loss: 0.4169 - val_accuracy: 0.8333\n",
      "Epoch 104/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4364 - accuracy: 0.7500\n",
      "Epoch 00104: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4349 - accuracy: 0.8417 - val_loss: 0.4154 - val_accuracy: 0.8333\n",
      "Epoch 105/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4645 - accuracy: 0.7500\n",
      "Epoch 00105: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4331 - accuracy: 0.8417 - val_loss: 0.4135 - val_accuracy: 0.8333\n",
      "Epoch 106/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4622 - accuracy: 0.7500\n",
      "Epoch 00106: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4316 - accuracy: 0.8417 - val_loss: 0.4120 - val_accuracy: 0.8333\n",
      "Epoch 107/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3804 - accuracy: 0.7500\n",
      "Epoch 00107: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4300 - accuracy: 0.8417 - val_loss: 0.4105 - val_accuracy: 0.8333\n",
      "Epoch 108/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4503 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4284 - accuracy: 0.8417 - val_loss: 0.4082 - val_accuracy: 0.8333\n",
      "Epoch 109/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5056 - accuracy: 0.7500\n",
      "Epoch 00109: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4263 - accuracy: 0.8417 - val_loss: 0.4067 - val_accuracy: 0.8333\n",
      "Epoch 110/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4554 - accuracy: 0.6667\n",
      "Epoch 00110: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4246 - accuracy: 0.8417 - val_loss: 0.4049 - val_accuracy: 0.8333\n",
      "Epoch 111/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3210 - accuracy: 0.9167\n",
      "Epoch 00111: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8500 - val_loss: 0.4030 - val_accuracy: 0.8333\n",
      "Epoch 112/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3883 - accuracy: 0.8333\n",
      "Epoch 00112: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.8500 - val_loss: 0.4013 - val_accuracy: 0.8333\n",
      "Epoch 113/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4135 - accuracy: 0.8333\n",
      "Epoch 00113: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4197 - accuracy: 0.8500 - val_loss: 0.3995 - val_accuracy: 0.8333\n",
      "Epoch 114/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5189 - accuracy: 0.8333\n",
      "Epoch 00114: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4183 - accuracy: 0.8500 - val_loss: 0.3977 - val_accuracy: 0.8333\n",
      "Epoch 115/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4799 - accuracy: 0.8333\n",
      "Epoch 00115: val_accuracy did not improve from 0.83333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.8500 - val_loss: 0.3960 - val_accuracy: 0.8333\n",
      "Epoch 116/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4041 - accuracy: 1.0000\n",
      "Epoch 00116: val_accuracy improved from 0.83333 to 0.86667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.4145 - accuracy: 0.8500 - val_loss: 0.3947 - val_accuracy: 0.8667\n",
      "Epoch 117/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4743 - accuracy: 0.8333\n",
      "Epoch 00117: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4127 - accuracy: 0.8667 - val_loss: 0.3932 - val_accuracy: 0.8667\n",
      "Epoch 118/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3301 - accuracy: 0.9167\n",
      "Epoch 00118: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4109 - accuracy: 0.8667 - val_loss: 0.3912 - val_accuracy: 0.8667\n",
      "Epoch 119/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3086 - accuracy: 0.9167\n",
      "Epoch 00119: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8667 - val_loss: 0.3894 - val_accuracy: 0.8667\n",
      "Epoch 120/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4634 - accuracy: 0.8333\n",
      "Epoch 00120: val_accuracy did not improve from 0.86667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4075 - accuracy: 0.8750 - val_loss: 0.3876 - val_accuracy: 0.8667\n",
      "Epoch 121/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4039 - accuracy: 0.7500\n",
      "Epoch 00121: val_accuracy improved from 0.86667 to 0.90000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.4061 - accuracy: 0.8750 - val_loss: 0.3862 - val_accuracy: 0.9000\n",
      "Epoch 122/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4042 - accuracy: 0.7500\n",
      "Epoch 00122: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4039 - accuracy: 0.8750 - val_loss: 0.3842 - val_accuracy: 0.9000\n",
      "Epoch 123/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4028 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4024 - accuracy: 0.8750 - val_loss: 0.3822 - val_accuracy: 0.9000\n",
      "Epoch 124/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5008 - accuracy: 0.6667\n",
      "Epoch 00124: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.4006 - accuracy: 0.8750 - val_loss: 0.3805 - val_accuracy: 0.9000\n",
      "Epoch 125/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3496 - accuracy: 0.9167\n",
      "Epoch 00125: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3990 - accuracy: 0.8750 - val_loss: 0.3788 - val_accuracy: 0.9000\n",
      "Epoch 126/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4640 - accuracy: 0.5833\n",
      "Epoch 00126: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3973 - accuracy: 0.8917 - val_loss: 0.3774 - val_accuracy: 0.9000\n",
      "Epoch 127/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2869 - accuracy: 0.9167\n",
      "Epoch 00127: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.8833 - val_loss: 0.3755 - val_accuracy: 0.9000\n",
      "Epoch 128/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5286 - accuracy: 0.5833\n",
      "Epoch 00128: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3938 - accuracy: 0.8917 - val_loss: 0.3738 - val_accuracy: 0.9000\n",
      "Epoch 129/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2259 - accuracy: 0.9167\n",
      "Epoch 00129: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8917 - val_loss: 0.3719 - val_accuracy: 0.9000\n",
      "Epoch 130/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3531 - accuracy: 1.0000\n",
      "Epoch 00130: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3903 - accuracy: 0.8917 - val_loss: 0.3702 - val_accuracy: 0.9000\n",
      "Epoch 131/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3346 - accuracy: 0.9167\n",
      "Epoch 00131: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3892 - accuracy: 0.8750 - val_loss: 0.3680 - val_accuracy: 0.9000\n",
      "Epoch 132/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3116 - accuracy: 0.9167\n",
      "Epoch 00132: val_accuracy did not improve from 0.90000\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3873 - accuracy: 0.9083 - val_loss: 0.3668 - val_accuracy: 0.9000\n",
      "Epoch 133/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4154 - accuracy: 0.9167\n",
      "Epoch 00133: val_accuracy improved from 0.90000 to 0.93333, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 0.3856 - accuracy: 0.9083 - val_loss: 0.3653 - val_accuracy: 0.9333\n",
      "Epoch 134/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4779 - accuracy: 0.8333\n",
      "Epoch 00134: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3834 - accuracy: 0.9167 - val_loss: 0.3636 - val_accuracy: 0.9333\n",
      "Epoch 135/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4171 - accuracy: 0.9167\n",
      "Epoch 00135: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.9083 - val_loss: 0.3615 - val_accuracy: 0.9000\n",
      "Epoch 136/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2602 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3801 - accuracy: 0.9083 - val_loss: 0.3600 - val_accuracy: 0.9333\n",
      "Epoch 137/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4202 - accuracy: 0.9167\n",
      "Epoch 00137: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3783 - accuracy: 0.9083 - val_loss: 0.3581 - val_accuracy: 0.9333\n",
      "Epoch 138/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3114 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3766 - accuracy: 0.9167 - val_loss: 0.3565 - val_accuracy: 0.9333\n",
      "Epoch 139/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4259 - accuracy: 0.8333\n",
      "Epoch 00139: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.9167 - val_loss: 0.3545 - val_accuracy: 0.9333\n",
      "Epoch 140/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3562 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3734 - accuracy: 0.9167 - val_loss: 0.3530 - val_accuracy: 0.9333\n",
      "Epoch 141/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3791 - accuracy: 0.7500\n",
      "Epoch 00141: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3718 - accuracy: 0.9167 - val_loss: 0.3517 - val_accuracy: 0.9333\n",
      "Epoch 142/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2607 - accuracy: 0.9167\n",
      "Epoch 00142: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.9167 - val_loss: 0.3496 - val_accuracy: 0.9333\n",
      "Epoch 143/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4364 - accuracy: 0.8333\n",
      "Epoch 00143: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.9167 - val_loss: 0.3477 - val_accuracy: 0.9333\n",
      "Epoch 144/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4289 - accuracy: 0.9167\n",
      "Epoch 00144: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3663 - accuracy: 0.9167 - val_loss: 0.3460 - val_accuracy: 0.9333\n",
      "Epoch 145/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3907 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3646 - accuracy: 0.9167 - val_loss: 0.3443 - val_accuracy: 0.9333\n",
      "Epoch 146/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3341 - accuracy: 0.8333\n",
      "Epoch 00146: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.9167 - val_loss: 0.3426 - val_accuracy: 0.9333\n",
      "Epoch 147/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1606 - accuracy: 0.9167\n",
      "Epoch 00147: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.9167 - val_loss: 0.3410 - val_accuracy: 0.9333\n",
      "Epoch 148/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3138 - accuracy: 0.8333\n",
      "Epoch 00148: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3596 - accuracy: 0.9333 - val_loss: 0.3395 - val_accuracy: 0.9333\n",
      "Epoch 149/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2533 - accuracy: 1.0000\n",
      "Epoch 00149: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3575 - accuracy: 0.9250 - val_loss: 0.3376 - val_accuracy: 0.9333\n",
      "Epoch 150/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2717 - accuracy: 1.0000\n",
      "Epoch 00150: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3559 - accuracy: 0.9250 - val_loss: 0.3356 - val_accuracy: 0.9333\n",
      "Epoch 151/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3168 - accuracy: 1.0000\n",
      "Epoch 00151: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3543 - accuracy: 0.9250 - val_loss: 0.3341 - val_accuracy: 0.9333\n",
      "Epoch 152/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4042 - accuracy: 0.9167\n",
      "Epoch 00152: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.9250 - val_loss: 0.3322 - val_accuracy: 0.9333\n",
      "Epoch 153/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3092 - accuracy: 0.9167\n",
      "Epoch 00153: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.9250 - val_loss: 0.3302 - val_accuracy: 0.9333\n",
      "Epoch 154/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3721 - accuracy: 1.0000\n",
      "Epoch 00154: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3490 - accuracy: 0.9250 - val_loss: 0.3285 - val_accuracy: 0.9333\n",
      "Epoch 155/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4633 - accuracy: 0.9167\n",
      "Epoch 00155: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.9250 - val_loss: 0.3269 - val_accuracy: 0.9333\n",
      "Epoch 156/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4329 - accuracy: 0.9167\n",
      "Epoch 00156: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3459 - accuracy: 0.9417 - val_loss: 0.3256 - val_accuracy: 0.9333\n",
      "Epoch 157/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2344 - accuracy: 0.9167\n",
      "Epoch 00157: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3438 - accuracy: 0.9500 - val_loss: 0.3239 - val_accuracy: 0.9333\n",
      "Epoch 158/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.5384 - accuracy: 0.9167\n",
      "Epoch 00158: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.9500 - val_loss: 0.3220 - val_accuracy: 0.9333\n",
      "Epoch 159/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2373 - accuracy: 0.9167\n",
      "Epoch 00159: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.9500 - val_loss: 0.3202 - val_accuracy: 0.9333\n",
      "Epoch 160/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4369 - accuracy: 0.8333\n",
      "Epoch 00160: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3388 - accuracy: 0.9500 - val_loss: 0.3186 - val_accuracy: 0.9333\n",
      "Epoch 161/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.4431 - accuracy: 1.0000\n",
      "Epoch 00161: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3370 - accuracy: 0.9417 - val_loss: 0.3167 - val_accuracy: 0.9333\n",
      "Epoch 162/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3849 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.9500 - val_loss: 0.3150 - val_accuracy: 0.9333\n",
      "Epoch 163/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3521 - accuracy: 1.0000\n",
      "Epoch 00163: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3339 - accuracy: 0.9333 - val_loss: 0.3132 - val_accuracy: 0.9333\n",
      "Epoch 164/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3859 - accuracy: 0.9167\n",
      "Epoch 00164: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.9417 - val_loss: 0.3116 - val_accuracy: 0.9333\n",
      "Epoch 165/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2903 - accuracy: 1.0000\n",
      "Epoch 00165: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3302 - accuracy: 0.9500 - val_loss: 0.3100 - val_accuracy: 0.9333\n",
      "Epoch 166/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3587 - accuracy: 1.0000\n",
      "Epoch 00166: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.9500 - val_loss: 0.3081 - val_accuracy: 0.9333\n",
      "Epoch 167/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3164 - accuracy: 1.0000\n",
      "Epoch 00167: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3274 - accuracy: 0.9583 - val_loss: 0.3068 - val_accuracy: 0.9333\n",
      "Epoch 168/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3308 - accuracy: 1.0000\n",
      "Epoch 00168: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3251 - accuracy: 0.9583 - val_loss: 0.3052 - val_accuracy: 0.9333\n",
      "Epoch 169/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2494 - accuracy: 1.0000\n",
      "Epoch 00169: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.9583 - val_loss: 0.3036 - val_accuracy: 0.9333\n",
      "Epoch 170/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2737 - accuracy: 0.9167\n",
      "Epoch 00170: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3218 - accuracy: 0.9583 - val_loss: 0.3019 - val_accuracy: 0.9333\n",
      "Epoch 171/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3513 - accuracy: 0.9167\n",
      "Epoch 00171: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.9583 - val_loss: 0.3001 - val_accuracy: 0.9333\n",
      "Epoch 172/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2856 - accuracy: 1.0000\n",
      "Epoch 00172: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3187 - accuracy: 0.9583 - val_loss: 0.2985 - val_accuracy: 0.9333\n",
      "Epoch 173/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1913 - accuracy: 0.9167\n",
      "Epoch 00173: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3169 - accuracy: 0.9583 - val_loss: 0.2969 - val_accuracy: 0.9333\n",
      "Epoch 174/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2730 - accuracy: 1.0000\n",
      "Epoch 00174: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.9583 - val_loss: 0.2952 - val_accuracy: 0.9333\n",
      "Epoch 175/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3157 - accuracy: 0.9167\n",
      "Epoch 00175: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3140 - accuracy: 0.9583 - val_loss: 0.2936 - val_accuracy: 0.9333\n",
      "Epoch 176/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2900 - accuracy: 1.0000\n",
      "Epoch 00176: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3127 - accuracy: 0.9583 - val_loss: 0.2919 - val_accuracy: 0.9333\n",
      "Epoch 177/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3580 - accuracy: 1.0000\n",
      "Epoch 00177: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3105 - accuracy: 0.9500 - val_loss: 0.2904 - val_accuracy: 0.9333\n",
      "Epoch 178/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3594 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.9583 - val_loss: 0.2890 - val_accuracy: 0.9333\n",
      "Epoch 179/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3223 - accuracy: 1.0000\n",
      "Epoch 00179: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3075 - accuracy: 0.9667 - val_loss: 0.2876 - val_accuracy: 0.9333\n",
      "Epoch 180/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3966 - accuracy: 0.9167\n",
      "Epoch 00180: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3060 - accuracy: 0.9583 - val_loss: 0.2859 - val_accuracy: 0.9333\n",
      "Epoch 181/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3066 - accuracy: 1.0000\n",
      "Epoch 00181: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3051 - accuracy: 0.9583 - val_loss: 0.2843 - val_accuracy: 0.9333\n",
      "Epoch 182/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3625 - accuracy: 1.0000\n",
      "Epoch 00182: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.9583 - val_loss: 0.2828 - val_accuracy: 0.9333\n",
      "Epoch 183/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3061 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3012 - accuracy: 0.9583 - val_loss: 0.2814 - val_accuracy: 0.9333\n",
      "Epoch 184/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1951 - accuracy: 1.0000\n",
      "Epoch 00184: val_accuracy did not improve from 0.93333\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2998 - accuracy: 0.9667 - val_loss: 0.2799 - val_accuracy: 0.9333\n",
      "Epoch 185/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2744 - accuracy: 1.0000\n",
      "Epoch 00185: val_accuracy improved from 0.93333 to 0.96667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2984 - accuracy: 0.9667 - val_loss: 0.2785 - val_accuracy: 0.9667\n",
      "Epoch 186/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3325 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.9667 - val_loss: 0.2770 - val_accuracy: 0.9667\n",
      "Epoch 187/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2912 - accuracy: 0.9167\n",
      "Epoch 00187: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.9667 - val_loss: 0.2756 - val_accuracy: 0.9667\n",
      "Epoch 188/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3947 - accuracy: 1.0000\n",
      "Epoch 00188: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2940 - accuracy: 0.9667 - val_loss: 0.2740 - val_accuracy: 0.9667\n",
      "Epoch 189/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2613 - accuracy: 1.0000\n",
      "Epoch 00189: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2924 - accuracy: 0.9667 - val_loss: 0.2724 - val_accuracy: 0.9333\n",
      "Epoch 190/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2814 - accuracy: 0.9167\n",
      "Epoch 00190: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.9667 - val_loss: 0.2710 - val_accuracy: 0.9333\n",
      "Epoch 191/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2730 - accuracy: 1.0000\n",
      "Epoch 00191: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2896 - accuracy: 0.9667 - val_loss: 0.2696 - val_accuracy: 0.9333\n",
      "Epoch 192/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2969 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2879 - accuracy: 0.9667 - val_loss: 0.2682 - val_accuracy: 0.9667\n",
      "Epoch 193/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1525 - accuracy: 1.0000\n",
      "Epoch 00193: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2864 - accuracy: 0.9667 - val_loss: 0.2668 - val_accuracy: 0.9667\n",
      "Epoch 194/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1933 - accuracy: 1.0000\n",
      "Epoch 00194: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2851 - accuracy: 0.9667 - val_loss: 0.2653 - val_accuracy: 0.9667\n",
      "Epoch 195/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3617 - accuracy: 0.8333\n",
      "Epoch 00195: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2836 - accuracy: 0.9667 - val_loss: 0.2639 - val_accuracy: 0.9667\n",
      "Epoch 196/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3836 - accuracy: 0.9167\n",
      "Epoch 00196: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2821 - accuracy: 0.9667 - val_loss: 0.2626 - val_accuracy: 0.9667\n",
      "Epoch 197/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2369 - accuracy: 1.0000\n",
      "Epoch 00197: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2809 - accuracy: 0.9667 - val_loss: 0.2612 - val_accuracy: 0.9667\n",
      "Epoch 198/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0927 - accuracy: 1.0000\n",
      "Epoch 00198: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2792 - accuracy: 0.9667 - val_loss: 0.2598 - val_accuracy: 0.9667\n",
      "Epoch 199/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3499 - accuracy: 0.9167\n",
      "Epoch 00199: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2780 - accuracy: 0.9667 - val_loss: 0.2584 - val_accuracy: 0.9667\n",
      "Epoch 200/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3564 - accuracy: 0.9167\n",
      "Epoch 00200: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2766 - accuracy: 0.9667 - val_loss: 0.2571 - val_accuracy: 0.9667\n",
      "Epoch 201/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3263 - accuracy: 0.9167\n",
      "Epoch 00201: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2751 - accuracy: 0.9667 - val_loss: 0.2557 - val_accuracy: 0.9667\n",
      "Epoch 202/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2774 - accuracy: 0.9167\n",
      "Epoch 00202: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2737 - accuracy: 0.9667 - val_loss: 0.2543 - val_accuracy: 0.9667\n",
      "Epoch 203/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3062 - accuracy: 0.9167\n",
      "Epoch 00203: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2724 - accuracy: 0.9667 - val_loss: 0.2530 - val_accuracy: 0.9667\n",
      "Epoch 204/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3130 - accuracy: 1.0000\n",
      "Epoch 00204: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2723 - accuracy: 0.9667 - val_loss: 0.2517 - val_accuracy: 0.9667\n",
      "Epoch 205/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3329 - accuracy: 1.0000\n",
      "Epoch 00205: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2696 - accuracy: 0.9667 - val_loss: 0.2503 - val_accuracy: 0.9667\n",
      "Epoch 206/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2497 - accuracy: 1.0000\n",
      "Epoch 00206: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2683 - accuracy: 0.9667 - val_loss: 0.2490 - val_accuracy: 0.9667\n",
      "Epoch 207/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3366 - accuracy: 1.0000\n",
      "Epoch 00207: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2670 - accuracy: 0.9667 - val_loss: 0.2477 - val_accuracy: 0.9667\n",
      "Epoch 208/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2568 - accuracy: 1.0000\n",
      "Epoch 00208: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2659 - accuracy: 0.9667 - val_loss: 0.2464 - val_accuracy: 0.9667\n",
      "Epoch 209/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1945 - accuracy: 1.0000\n",
      "Epoch 00209: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2642 - accuracy: 0.9667 - val_loss: 0.2451 - val_accuracy: 0.9667\n",
      "Epoch 210/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1670 - accuracy: 1.0000\n",
      "Epoch 00210: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2634 - accuracy: 0.9667 - val_loss: 0.2438 - val_accuracy: 0.9667\n",
      "Epoch 211/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2290 - accuracy: 1.0000\n",
      "Epoch 00211: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2617 - accuracy: 0.9667 - val_loss: 0.2425 - val_accuracy: 0.9667\n",
      "Epoch 212/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2595 - accuracy: 0.9167\n",
      "Epoch 00212: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2604 - accuracy: 0.9667 - val_loss: 0.2412 - val_accuracy: 0.9667\n",
      "Epoch 213/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2627 - accuracy: 1.0000\n",
      "Epoch 00213: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2591 - accuracy: 0.9667 - val_loss: 0.2399 - val_accuracy: 0.9667\n",
      "Epoch 214/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2861 - accuracy: 1.0000\n",
      "Epoch 00214: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2578 - accuracy: 0.9667 - val_loss: 0.2387 - val_accuracy: 0.9667\n",
      "Epoch 215/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1993 - accuracy: 1.0000\n",
      "Epoch 00215: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.9667 - val_loss: 0.2374 - val_accuracy: 0.9667\n",
      "Epoch 216/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3489 - accuracy: 0.9167\n",
      "Epoch 00216: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2553 - accuracy: 0.9667 - val_loss: 0.2362 - val_accuracy: 0.9667\n",
      "Epoch 217/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2541 - accuracy: 0.9167\n",
      "Epoch 00217: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.9667 - val_loss: 0.2350 - val_accuracy: 0.9667\n",
      "Epoch 218/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2367 - accuracy: 1.0000\n",
      "Epoch 00218: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.9667 - val_loss: 0.2338 - val_accuracy: 0.9667\n",
      "Epoch 219/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2954 - accuracy: 1.0000\n",
      "Epoch 00219: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2518 - accuracy: 0.9667 - val_loss: 0.2326 - val_accuracy: 0.9667\n",
      "Epoch 220/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3186 - accuracy: 1.0000\n",
      "Epoch 00220: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2503 - accuracy: 0.9667 - val_loss: 0.2314 - val_accuracy: 0.9667\n",
      "Epoch 221/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3393 - accuracy: 0.9167\n",
      "Epoch 00221: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2492 - accuracy: 0.9667 - val_loss: 0.2302 - val_accuracy: 0.9667\n",
      "Epoch 222/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1140 - accuracy: 1.0000\n",
      "Epoch 00222: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2478 - accuracy: 0.9667 - val_loss: 0.2290 - val_accuracy: 0.9667\n",
      "Epoch 223/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2333 - accuracy: 0.9167\n",
      "Epoch 00223: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.9667 - val_loss: 0.2278 - val_accuracy: 0.9667\n",
      "Epoch 224/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3480 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.2460 - accuracy: 0.9667 - val_loss: 0.2267 - val_accuracy: 0.9667\n",
      "Epoch 225/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1682 - accuracy: 1.0000\n",
      "Epoch 00225: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.9667 - val_loss: 0.2254 - val_accuracy: 0.9667\n",
      "Epoch 226/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2988 - accuracy: 1.0000\n",
      "Epoch 00226: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2430 - accuracy: 0.9667 - val_loss: 0.2243 - val_accuracy: 0.9667\n",
      "Epoch 227/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1939 - accuracy: 1.0000\n",
      "Epoch 00227: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2418 - accuracy: 0.9667 - val_loss: 0.2232 - val_accuracy: 0.9667\n",
      "Epoch 228/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3008 - accuracy: 0.9167\n",
      "Epoch 00228: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2406 - accuracy: 0.9667 - val_loss: 0.2220 - val_accuracy: 0.9667\n",
      "Epoch 229/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2686 - accuracy: 0.9167\n",
      "Epoch 00229: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2398 - accuracy: 0.9667 - val_loss: 0.2211 - val_accuracy: 0.9667\n",
      "Epoch 230/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1877 - accuracy: 1.0000\n",
      "Epoch 00230: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2405 - accuracy: 0.9583 - val_loss: 0.2198 - val_accuracy: 0.9667\n",
      "Epoch 231/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1728 - accuracy: 1.0000\n",
      "Epoch 00231: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2373 - accuracy: 0.9583 - val_loss: 0.2186 - val_accuracy: 0.9667\n",
      "Epoch 232/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3268 - accuracy: 1.0000\n",
      "Epoch 00232: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2370 - accuracy: 0.9667 - val_loss: 0.2177 - val_accuracy: 0.9667\n",
      "Epoch 233/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2163 - accuracy: 0.9167\n",
      "Epoch 00233: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2351 - accuracy: 0.9667 - val_loss: 0.2165 - val_accuracy: 0.9667\n",
      "Epoch 234/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2285 - accuracy: 1.0000\n",
      "Epoch 00234: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.9667 - val_loss: 0.2154 - val_accuracy: 0.9667\n",
      "Epoch 235/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2200 - accuracy: 1.0000\n",
      "Epoch 00235: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.9667 - val_loss: 0.2144 - val_accuracy: 0.9667\n",
      "Epoch 236/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1360 - accuracy: 1.0000\n",
      "Epoch 00236: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2326 - accuracy: 0.9667 - val_loss: 0.2132 - val_accuracy: 0.9667\n",
      "Epoch 237/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1803 - accuracy: 1.0000\n",
      "Epoch 00237: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2311 - accuracy: 0.9667 - val_loss: 0.2124 - val_accuracy: 0.9667\n",
      "Epoch 238/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2827 - accuracy: 1.0000\n",
      "Epoch 00238: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.9667 - val_loss: 0.2112 - val_accuracy: 0.9667\n",
      "Epoch 239/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3298 - accuracy: 0.7500\n",
      "Epoch 00239: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2286 - accuracy: 0.9667 - val_loss: 0.2101 - val_accuracy: 0.9667\n",
      "Epoch 240/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1880 - accuracy: 1.0000\n",
      "Epoch 00240: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2273 - accuracy: 0.9667 - val_loss: 0.2093 - val_accuracy: 0.9667\n",
      "Epoch 241/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2074 - accuracy: 1.0000\n",
      "Epoch 00241: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2261 - accuracy: 0.9667 - val_loss: 0.2083 - val_accuracy: 0.9667\n",
      "Epoch 242/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2288 - accuracy: 0.9167\n",
      "Epoch 00242: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2251 - accuracy: 0.9667 - val_loss: 0.2073 - val_accuracy: 0.9667\n",
      "Epoch 243/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2352 - accuracy: 1.0000\n",
      "Epoch 00243: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2240 - accuracy: 0.9667 - val_loss: 0.2061 - val_accuracy: 0.9667\n",
      "Epoch 244/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1982 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2229 - accuracy: 0.9667 - val_loss: 0.2051 - val_accuracy: 0.9667\n",
      "Epoch 245/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2601 - accuracy: 0.9167\n",
      "Epoch 00245: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2221 - accuracy: 0.9667 - val_loss: 0.2041 - val_accuracy: 0.9667\n",
      "Epoch 246/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2160 - accuracy: 0.9167\n",
      "Epoch 00246: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2209 - accuracy: 0.9667 - val_loss: 0.2032 - val_accuracy: 0.9667\n",
      "Epoch 247/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2908 - accuracy: 0.9167\n",
      "Epoch 00247: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2200 - accuracy: 0.9667 - val_loss: 0.2023 - val_accuracy: 0.9667\n",
      "Epoch 248/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2106 - accuracy: 1.0000\n",
      "Epoch 00248: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2189 - accuracy: 0.9667 - val_loss: 0.2013 - val_accuracy: 0.9667\n",
      "Epoch 249/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1466 - accuracy: 0.9167\n",
      "Epoch 00249: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2186 - accuracy: 0.9583 - val_loss: 0.2002 - val_accuracy: 0.9667\n",
      "Epoch 250/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2798 - accuracy: 0.9167\n",
      "Epoch 00250: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.9583 - val_loss: 0.1992 - val_accuracy: 0.9667\n",
      "Epoch 251/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1120 - accuracy: 1.0000\n",
      "Epoch 00251: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2158 - accuracy: 0.9667 - val_loss: 0.1984 - val_accuracy: 0.9667\n",
      "Epoch 252/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2810 - accuracy: 0.9167\n",
      "Epoch 00252: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.9667 - val_loss: 0.1977 - val_accuracy: 0.9667\n",
      "Epoch 253/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2328 - accuracy: 0.9167\n",
      "Epoch 00253: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2139 - accuracy: 0.9667 - val_loss: 0.1966 - val_accuracy: 0.9667\n",
      "Epoch 254/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2429 - accuracy: 1.0000\n",
      "Epoch 00254: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2130 - accuracy: 0.9667 - val_loss: 0.1958 - val_accuracy: 0.9667\n",
      "Epoch 255/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1703 - accuracy: 1.0000\n",
      "Epoch 00255: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.9667 - val_loss: 0.1948 - val_accuracy: 0.9667\n",
      "Epoch 256/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2097 - accuracy: 1.0000\n",
      "Epoch 00256: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2111 - accuracy: 0.9667 - val_loss: 0.1937 - val_accuracy: 0.9667\n",
      "Epoch 257/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1803 - accuracy: 1.0000\n",
      "Epoch 00257: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2106 - accuracy: 0.9667 - val_loss: 0.1931 - val_accuracy: 0.9667\n",
      "Epoch 258/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2794 - accuracy: 1.0000\n",
      "Epoch 00258: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2094 - accuracy: 0.9583 - val_loss: 0.1919 - val_accuracy: 0.9667\n",
      "Epoch 259/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2913 - accuracy: 0.9167\n",
      "Epoch 00259: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9583 - val_loss: 0.1910 - val_accuracy: 0.9667\n",
      "Epoch 260/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2148 - accuracy: 1.0000\n",
      "Epoch 00260: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2075 - accuracy: 0.9583 - val_loss: 0.1904 - val_accuracy: 0.9667\n",
      "Epoch 261/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2299 - accuracy: 0.9167\n",
      "Epoch 00261: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2066 - accuracy: 0.9667 - val_loss: 0.1897 - val_accuracy: 0.9667\n",
      "Epoch 262/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2149 - accuracy: 1.0000\n",
      "Epoch 00262: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2055 - accuracy: 0.9667 - val_loss: 0.1887 - val_accuracy: 0.9667\n",
      "Epoch 263/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2763 - accuracy: 0.9167\n",
      "Epoch 00263: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9667 - val_loss: 0.1878 - val_accuracy: 0.9667\n",
      "Epoch 264/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1609 - accuracy: 0.9167\n",
      "Epoch 00264: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9583 - val_loss: 0.1869 - val_accuracy: 0.9667\n",
      "Epoch 265/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1372 - accuracy: 1.0000\n",
      "Epoch 00265: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2032 - accuracy: 0.9583 - val_loss: 0.1859 - val_accuracy: 0.9667\n",
      "Epoch 266/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1868 - accuracy: 1.0000\n",
      "Epoch 00266: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2021 - accuracy: 0.9583 - val_loss: 0.1853 - val_accuracy: 0.9667\n",
      "Epoch 267/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1814 - accuracy: 1.0000\n",
      "Epoch 00267: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.9667 - val_loss: 0.1846 - val_accuracy: 0.9667\n",
      "Epoch 268/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1917 - accuracy: 0.9167\n",
      "Epoch 00268: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.2002 - accuracy: 0.9583 - val_loss: 0.1836 - val_accuracy: 0.9667\n",
      "Epoch 269/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.0932 - accuracy: 1.0000\n",
      "Epoch 00269: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1995 - accuracy: 0.9583 - val_loss: 0.1829 - val_accuracy: 0.9667\n",
      "Epoch 270/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2955 - accuracy: 0.9167\n",
      "Epoch 00270: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1983 - accuracy: 0.9667 - val_loss: 0.1820 - val_accuracy: 0.9667\n",
      "Epoch 271/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2676 - accuracy: 0.9167\n",
      "Epoch 00271: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9583 - val_loss: 0.1812 - val_accuracy: 0.9667\n",
      "Epoch 272/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1818 - accuracy: 0.9167\n",
      "Epoch 00272: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1968 - accuracy: 0.9583 - val_loss: 0.1803 - val_accuracy: 0.9667\n",
      "Epoch 273/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2247 - accuracy: 1.0000\n",
      "Epoch 00273: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1959 - accuracy: 0.9583 - val_loss: 0.1796 - val_accuracy: 0.9667\n",
      "Epoch 274/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2123 - accuracy: 1.0000\n",
      "Epoch 00274: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1950 - accuracy: 0.9583 - val_loss: 0.1789 - val_accuracy: 0.9667\n",
      "Epoch 275/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3123 - accuracy: 0.8333\n",
      "Epoch 00275: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1943 - accuracy: 0.9583 - val_loss: 0.1782 - val_accuracy: 0.9667\n",
      "Epoch 276/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1045 - accuracy: 1.0000\n",
      "Epoch 00276: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9583 - val_loss: 0.1774 - val_accuracy: 0.9667\n",
      "Epoch 277/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1900 - accuracy: 0.9167\n",
      "Epoch 00277: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1927 - accuracy: 0.9583 - val_loss: 0.1768 - val_accuracy: 0.9667\n",
      "Epoch 278/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1550 - accuracy: 1.0000\n",
      "Epoch 00278: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1919 - accuracy: 0.9583 - val_loss: 0.1758 - val_accuracy: 0.9667\n",
      "Epoch 279/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1194 - accuracy: 1.0000\n",
      "Epoch 00279: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1910 - accuracy: 0.9583 - val_loss: 0.1751 - val_accuracy: 0.9667\n",
      "Epoch 280/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2529 - accuracy: 0.9167\n",
      "Epoch 00280: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1902 - accuracy: 0.9583 - val_loss: 0.1744 - val_accuracy: 0.9667\n",
      "Epoch 281/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.3050 - accuracy: 0.7500\n",
      "Epoch 00281: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1894 - accuracy: 0.9583 - val_loss: 0.1735 - val_accuracy: 0.9667\n",
      "Epoch 282/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1382 - accuracy: 1.0000\n",
      "Epoch 00282: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1886 - accuracy: 0.9583 - val_loss: 0.1729 - val_accuracy: 0.9667\n",
      "Epoch 283/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1696 - accuracy: 0.9167\n",
      "Epoch 00283: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1879 - accuracy: 0.9583 - val_loss: 0.1721 - val_accuracy: 0.9667\n",
      "Epoch 284/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1748 - accuracy: 0.9167\n",
      "Epoch 00284: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1871 - accuracy: 0.9667 - val_loss: 0.1716 - val_accuracy: 0.9667\n",
      "Epoch 285/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1591 - accuracy: 1.0000\n",
      "Epoch 00285: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9667 - val_loss: 0.1709 - val_accuracy: 0.9667\n",
      "Epoch 286/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1641 - accuracy: 0.9167\n",
      "Epoch 00286: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1858 - accuracy: 0.9667 - val_loss: 0.1699 - val_accuracy: 0.9667\n",
      "Epoch 287/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1730 - accuracy: 0.9167\n",
      "Epoch 00287: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9583 - val_loss: 0.1692 - val_accuracy: 0.9667\n",
      "Epoch 288/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1764 - accuracy: 1.0000\n",
      "Epoch 00288: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9583 - val_loss: 0.1688 - val_accuracy: 0.9667\n",
      "Epoch 289/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1687 - accuracy: 1.0000\n",
      "Epoch 00289: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1837 - accuracy: 0.9667 - val_loss: 0.1681 - val_accuracy: 0.9667\n",
      "Epoch 290/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2750 - accuracy: 1.0000\n",
      "Epoch 00290: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.9667 - val_loss: 0.1674 - val_accuracy: 0.9667\n",
      "Epoch 291/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1520 - accuracy: 1.0000\n",
      "Epoch 00291: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1826 - accuracy: 0.9583 - val_loss: 0.1664 - val_accuracy: 0.9667\n",
      "Epoch 292/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1889 - accuracy: 0.9167\n",
      "Epoch 00292: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1812 - accuracy: 0.9583 - val_loss: 0.1658 - val_accuracy: 0.9667\n",
      "Epoch 293/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1181 - accuracy: 1.0000\n",
      "Epoch 00293: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1803 - accuracy: 0.9667 - val_loss: 0.1656 - val_accuracy: 0.9667\n",
      "Epoch 294/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2613 - accuracy: 1.0000\n",
      "Epoch 00294: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9750 - val_loss: 0.1653 - val_accuracy: 0.9667\n",
      "Epoch 295/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2097 - accuracy: 1.0000\n",
      "Epoch 00295: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1791 - accuracy: 0.9667 - val_loss: 0.1643 - val_accuracy: 0.9667\n",
      "Epoch 296/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.2499 - accuracy: 0.9167\n",
      "Epoch 00296: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1785 - accuracy: 0.9750 - val_loss: 0.1639 - val_accuracy: 0.9667\n",
      "Epoch 297/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1236 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1775 - accuracy: 0.9667 - val_loss: 0.1627 - val_accuracy: 0.9667\n",
      "Epoch 298/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1076 - accuracy: 1.0000\n",
      "Epoch 00298: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1768 - accuracy: 0.9667 - val_loss: 0.1619 - val_accuracy: 0.9667\n",
      "Epoch 299/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1921 - accuracy: 1.0000\n",
      "Epoch 00299: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1764 - accuracy: 0.9667 - val_loss: 0.1616 - val_accuracy: 0.9667\n",
      "Epoch 300/300\n",
      " 1/10 [==>...........................] - ETA: 0s - loss: 0.1559 - accuracy: 1.0000\n",
      "Epoch 00300: val_accuracy did not improve from 0.96667\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.1754 - accuracy: 0.9667 - val_loss: 0.1609 - val_accuracy: 0.9667\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gráfico comparativo da Acurácia e Perda no treinamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "plot_history(history)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apresentando a Acurácia final do Treino, carregando o melhor modelo salvo e apresentando a Acurácia do Teste"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "scores = model.evaluate(X_train, y_train)\r\n",
    "print()\r\n",
    "print(f\"Acuracia do Treino: {round(scores[1]*100,2)}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9667\n",
      "\n",
      "Acuracia do Treino: 96.67%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model = load_model(\"./modelo_mlp_ex3_1.hdf5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "scores = model.evaluate(X_test, y_test)\r\n",
    "print()\r\n",
    "print(f\"Acuracia do Teste: {round(scores[1]*100,2)}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 998us/step - loss: 0.2785 - accuracy: 0.9667\n",
      "\n",
      "Acuracia da Validação: 96.67%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Verificamos ao final uma Acurácia de mais de 95% na base de treino, mesmo com uma rede bem pequena. \r\n",
    "## Acreditamos que este resultado é possível porque os dados são muito bem definidos, como podemos observar no gráfico das variáveis, permitindo que a rede consiga separar bem a fronteira entra cada uma das respostas."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c24c9dacf042e5cf8b743bae11b2cef3a95983df3bc5153773d9ffef1d5207d2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}