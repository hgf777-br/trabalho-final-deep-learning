{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bibliotecas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from sklearn import datasets\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\r\n",
    "\r\n",
    "from keras.models import Sequential, load_model\r\n",
    "from keras.layers import Dense\r\n",
    "from keras.layers import Dropout\r\n",
    "from keras.utils.vis_utils import plot_model\r\n",
    "from keras.utils.np_utils import  to_categorical\r\n",
    "from keras.callbacks import ModelCheckpoint\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Funções Auxiliares"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def plot_history(history):\r\n",
    "    acc = history.history['accuracy']\r\n",
    "    val_acc = history.history['val_accuracy']\r\n",
    "    loss = history.history['loss']\r\n",
    "    val_loss = history.history['val_loss']\r\n",
    "    x = range(1, len(acc) + 1)\r\n",
    "\r\n",
    "    plt.figure(figsize=(12, 5))\r\n",
    "    plt.subplot(1, 2, 1)\r\n",
    "    plt.plot(x, acc, 'b', label='Training Accuracy')\r\n",
    "    plt.plot(x, val_acc, 'r', label='Validation Accuracy')\r\n",
    "    plt.title('Accuracy')\r\n",
    "    plt.legend()\r\n",
    "    plt.subplot(1, 2, 2)\r\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\r\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\r\n",
    "    plt.title('Loss')\r\n",
    "    plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Análise dos Dados\r\n",
    "\r\n",
    "## Carregando a base de dados"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "iris = datasets.load_iris()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Criando os dados para o treino."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "features = iris.feature_names\r\n",
    "X = iris.data\r\n",
    "X[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "classes = iris.target_names\r\n",
    "y = iris.target\r\n",
    "y[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analisando a distribuição da variável resposta"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "sns.countplot(x = y)\r\n",
    "plt.title('Distribuição da variável Target')\r\n",
    "plt.xlabel('Target')\r\n",
    "plt.xticks(ticks=range(0,3), labels=classes)\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padronizando os dados com o StandardScaler do SKlearn. \r\n",
    "### Dados padronizados entre 0 e 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "scaler = StandardScaler()\r\n",
    "X = scaler.fit_transform(X)\r\n",
    "X[:5]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.90068117,  1.01900435, -1.34022653, -1.3154443 ],\n",
       "       [-1.14301691, -0.13197948, -1.34022653, -1.3154443 ],\n",
       "       [-1.38535265,  0.32841405, -1.39706395, -1.3154443 ],\n",
       "       [-1.50652052,  0.09821729, -1.2833891 , -1.3154443 ],\n",
       "       [-1.02184904,  1.24920112, -1.34022653, -1.3154443 ]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformando a variável resposta em \"One-hot vector\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "num_features = X.shape[1]\r\n",
    "y = to_categorical(y)\r\n",
    "num_classes = y.shape[1]\r\n",
    "print(y.shape)\r\n",
    "y[:5]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(150, 3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dividindo a Base de Dados entre 80% Treino e 20% Teste."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "X_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "y_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(120, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelo de MLP\r\n",
    "\r\n",
    "## Criando o modelo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Dense(8, input_dim=num_features, activation='relu'))\r\n",
    "model.add(Dense(num_classes , activation='softmax'))\r\n",
    "print(model.summary())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.Image object>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compilando e treinando o modelo. \r\n",
    "### Vamos utilizar a função de Callback ModelCheckPointer para salvar o modelo com a melhor accuracia na base de validação (que é a mesma de teste final pois temos uma base muito pequena)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "%%time\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "\r\n",
    "checkpointer = ModelCheckpoint(filepath='./modelo_mlp_ex3_1.hdf5', verbose=1,  save_best_only=True, monitor='val_accuracy')\r\n",
    "\r\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=16, callbacks=[checkpointer])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.2629 - accuracy: 0.3125\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.33333, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.1953 - accuracy: 0.3333 - val_loss: 1.2144 - val_accuracy: 0.3333\n",
      "Epoch 2/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.1935 - accuracy: 0.1875\n",
      "Epoch 00002: val_accuracy did not improve from 0.33333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.1587 - accuracy: 0.3333 - val_loss: 1.1752 - val_accuracy: 0.3333\n",
      "Epoch 3/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0723 - accuracy: 0.5000\n",
      "Epoch 00003: val_accuracy did not improve from 0.33333\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.1242 - accuracy: 0.3333 - val_loss: 1.1388 - val_accuracy: 0.3333\n",
      "Epoch 4/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0330 - accuracy: 0.4375\n",
      "Epoch 00004: val_accuracy did not improve from 0.33333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0920 - accuracy: 0.3333 - val_loss: 1.1028 - val_accuracy: 0.3333\n",
      "Epoch 5/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0297 - accuracy: 0.3750\n",
      "Epoch 00005: val_accuracy did not improve from 0.33333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0605 - accuracy: 0.3417 - val_loss: 1.0683 - val_accuracy: 0.3333\n",
      "Epoch 6/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0724 - accuracy: 0.3125\n",
      "Epoch 00006: val_accuracy did not improve from 0.33333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0306 - accuracy: 0.3333 - val_loss: 1.0350 - val_accuracy: 0.3333\n",
      "Epoch 7/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0805 - accuracy: 0.1875\n",
      "Epoch 00007: val_accuracy did not improve from 0.33333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 1.0025 - accuracy: 0.3417 - val_loss: 1.0036 - val_accuracy: 0.3333\n",
      "Epoch 8/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9156 - accuracy: 0.6250\n",
      "Epoch 00008: val_accuracy improved from 0.33333 to 0.36667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9755 - accuracy: 0.4250 - val_loss: 0.9736 - val_accuracy: 0.3667\n",
      "Epoch 9/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 1.0582 - accuracy: 0.3125\n",
      "Epoch 00009: val_accuracy improved from 0.36667 to 0.50000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9494 - accuracy: 0.4917 - val_loss: 0.9451 - val_accuracy: 0.5000\n",
      "Epoch 10/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8737 - accuracy: 0.5625\n",
      "Epoch 00010: val_accuracy improved from 0.50000 to 0.60000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9253 - accuracy: 0.5750 - val_loss: 0.9166 - val_accuracy: 0.6000\n",
      "Epoch 11/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9191 - accuracy: 0.6250\n",
      "Epoch 00011: val_accuracy improved from 0.60000 to 0.66667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.9011 - accuracy: 0.6250 - val_loss: 0.8891 - val_accuracy: 0.6667\n",
      "Epoch 12/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9131 - accuracy: 0.7500\n",
      "Epoch 00012: val_accuracy improved from 0.66667 to 0.76667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8775 - accuracy: 0.6750 - val_loss: 0.8637 - val_accuracy: 0.7667\n",
      "Epoch 13/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8560 - accuracy: 0.8125\n",
      "Epoch 00013: val_accuracy did not improve from 0.76667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8566 - accuracy: 0.7167 - val_loss: 0.8383 - val_accuracy: 0.7667\n",
      "Epoch 14/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7617 - accuracy: 0.9375\n",
      "Epoch 00014: val_accuracy did not improve from 0.76667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.8348 - accuracy: 0.7833 - val_loss: 0.8146 - val_accuracy: 0.7667\n",
      "Epoch 15/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9005 - accuracy: 0.7500\n",
      "Epoch 00015: val_accuracy improved from 0.76667 to 0.80000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.8140 - accuracy: 0.8083 - val_loss: 0.7921 - val_accuracy: 0.8000\n",
      "Epoch 16/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.9202 - accuracy: 0.6875\n",
      "Epoch 00016: val_accuracy improved from 0.80000 to 0.83333, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7945 - accuracy: 0.8083 - val_loss: 0.7694 - val_accuracy: 0.8333\n",
      "Epoch 17/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8464 - accuracy: 0.6875\n",
      "Epoch 00017: val_accuracy improved from 0.83333 to 0.86667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.7751 - accuracy: 0.8333 - val_loss: 0.7477 - val_accuracy: 0.8667\n",
      "Epoch 18/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7630 - accuracy: 0.8750\n",
      "Epoch 00018: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.7571 - accuracy: 0.8500 - val_loss: 0.7262 - val_accuracy: 0.8667\n",
      "Epoch 19/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8436 - accuracy: 0.6875\n",
      "Epoch 00019: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.7391 - accuracy: 0.8500 - val_loss: 0.7056 - val_accuracy: 0.8667\n",
      "Epoch 20/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7374 - accuracy: 0.8125\n",
      "Epoch 00020: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.7218 - accuracy: 0.8500 - val_loss: 0.6864 - val_accuracy: 0.8667\n",
      "Epoch 21/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.7678 - accuracy: 0.8750\n",
      "Epoch 00021: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.7053 - accuracy: 0.8333 - val_loss: 0.6682 - val_accuracy: 0.8667\n",
      "Epoch 22/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.8106 - accuracy: 0.8125\n",
      "Epoch 00022: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6897 - accuracy: 0.8333 - val_loss: 0.6502 - val_accuracy: 0.8667\n",
      "Epoch 23/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6596 - accuracy: 0.8750\n",
      "Epoch 00023: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6744 - accuracy: 0.8250 - val_loss: 0.6332 - val_accuracy: 0.8667\n",
      "Epoch 24/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6533 - accuracy: 0.8750\n",
      "Epoch 00024: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6599 - accuracy: 0.8250 - val_loss: 0.6167 - val_accuracy: 0.8667\n",
      "Epoch 25/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6377 - accuracy: 0.8125\n",
      "Epoch 00025: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6459 - accuracy: 0.8250 - val_loss: 0.6012 - val_accuracy: 0.8667\n",
      "Epoch 26/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6479 - accuracy: 0.8125\n",
      "Epoch 00026: val_accuracy did not improve from 0.86667\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6325 - accuracy: 0.8250 - val_loss: 0.5864 - val_accuracy: 0.8667\n",
      "Epoch 27/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6000 - accuracy: 0.8125\n",
      "Epoch 00027: val_accuracy improved from 0.86667 to 0.90000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.6197 - accuracy: 0.8250 - val_loss: 0.5720 - val_accuracy: 0.9000\n",
      "Epoch 28/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6300 - accuracy: 0.8125\n",
      "Epoch 00028: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.6073 - accuracy: 0.8250 - val_loss: 0.5582 - val_accuracy: 0.9000\n",
      "Epoch 29/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4136 - accuracy: 1.0000\n",
      "Epoch 00029: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5952 - accuracy: 0.8250 - val_loss: 0.5450 - val_accuracy: 0.9000\n",
      "Epoch 30/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4866 - accuracy: 0.8125\n",
      "Epoch 00030: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5836 - accuracy: 0.8250 - val_loss: 0.5324 - val_accuracy: 0.9000\n",
      "Epoch 31/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.6792 - accuracy: 0.6875\n",
      "Epoch 00031: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5725 - accuracy: 0.8250 - val_loss: 0.5201 - val_accuracy: 0.9000\n",
      "Epoch 32/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5381 - accuracy: 0.8125\n",
      "Epoch 00032: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5618 - accuracy: 0.8417 - val_loss: 0.5085 - val_accuracy: 0.9000\n",
      "Epoch 33/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4569 - accuracy: 1.0000\n",
      "Epoch 00033: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5512 - accuracy: 0.8417 - val_loss: 0.4978 - val_accuracy: 0.9000\n",
      "Epoch 34/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5585 - accuracy: 0.7500\n",
      "Epoch 00034: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5414 - accuracy: 0.8417 - val_loss: 0.4874 - val_accuracy: 0.9000\n",
      "Epoch 35/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3328 - accuracy: 1.0000\n",
      "Epoch 00035: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5317 - accuracy: 0.8500 - val_loss: 0.4772 - val_accuracy: 0.9000\n",
      "Epoch 36/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4517 - accuracy: 0.7500\n",
      "Epoch 00036: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5223 - accuracy: 0.8500 - val_loss: 0.4673 - val_accuracy: 0.9000\n",
      "Epoch 37/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3983 - accuracy: 0.8125\n",
      "Epoch 00037: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5134 - accuracy: 0.8500 - val_loss: 0.4579 - val_accuracy: 0.9000\n",
      "Epoch 38/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4360 - accuracy: 0.8750\n",
      "Epoch 00038: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5046 - accuracy: 0.8500 - val_loss: 0.4489 - val_accuracy: 0.9000\n",
      "Epoch 39/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5051 - accuracy: 0.8125\n",
      "Epoch 00039: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4964 - accuracy: 0.8583 - val_loss: 0.4402 - val_accuracy: 0.9000\n",
      "Epoch 40/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4172 - accuracy: 0.8750\n",
      "Epoch 00040: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4877 - accuracy: 0.8667 - val_loss: 0.4319 - val_accuracy: 0.9000\n",
      "Epoch 41/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5115 - accuracy: 0.9375\n",
      "Epoch 00041: val_accuracy did not improve from 0.90000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4796 - accuracy: 0.8750 - val_loss: 0.4240 - val_accuracy: 0.9000\n",
      "Epoch 42/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5252 - accuracy: 0.7500\n",
      "Epoch 00042: val_accuracy improved from 0.90000 to 0.93333, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.4728 - accuracy: 0.8750 - val_loss: 0.4168 - val_accuracy: 0.9333\n",
      "Epoch 43/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3836 - accuracy: 0.8750\n",
      "Epoch 00043: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4641 - accuracy: 0.8833 - val_loss: 0.4089 - val_accuracy: 0.9333\n",
      "Epoch 44/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5260 - accuracy: 0.8125\n",
      "Epoch 00044: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4567 - accuracy: 0.8917 - val_loss: 0.4011 - val_accuracy: 0.9333\n",
      "Epoch 45/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3715 - accuracy: 0.9375\n",
      "Epoch 00045: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4495 - accuracy: 0.8917 - val_loss: 0.3936 - val_accuracy: 0.9333\n",
      "Epoch 46/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4207 - accuracy: 0.9375\n",
      "Epoch 00046: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4426 - accuracy: 0.8917 - val_loss: 0.3865 - val_accuracy: 0.9333\n",
      "Epoch 47/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3364 - accuracy: 1.0000\n",
      "Epoch 00047: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4358 - accuracy: 0.8917 - val_loss: 0.3795 - val_accuracy: 0.9333\n",
      "Epoch 48/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3256 - accuracy: 1.0000\n",
      "Epoch 00048: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.8917 - val_loss: 0.3730 - val_accuracy: 0.9333\n",
      "Epoch 49/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5101 - accuracy: 0.8125\n",
      "Epoch 00049: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4226 - accuracy: 0.8917 - val_loss: 0.3667 - val_accuracy: 0.9333\n",
      "Epoch 50/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4446 - accuracy: 0.8750\n",
      "Epoch 00050: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4160 - accuracy: 0.8917 - val_loss: 0.3604 - val_accuracy: 0.9333\n",
      "Epoch 51/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3570 - accuracy: 0.9375\n",
      "Epoch 00051: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4095 - accuracy: 0.9000 - val_loss: 0.3542 - val_accuracy: 0.9333\n",
      "Epoch 52/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.5376 - accuracy: 0.8125\n",
      "Epoch 00052: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.4034 - accuracy: 0.9000 - val_loss: 0.3483 - val_accuracy: 0.9333\n",
      "Epoch 53/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4080 - accuracy: 0.9375\n",
      "Epoch 00053: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3972 - accuracy: 0.9083 - val_loss: 0.3425 - val_accuracy: 0.9333\n",
      "Epoch 54/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3701 - accuracy: 0.8750\n",
      "Epoch 00054: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3915 - accuracy: 0.9167 - val_loss: 0.3369 - val_accuracy: 0.9333\n",
      "Epoch 55/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3159 - accuracy: 0.9375\n",
      "Epoch 00055: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3856 - accuracy: 0.9167 - val_loss: 0.3313 - val_accuracy: 0.9333\n",
      "Epoch 56/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3221 - accuracy: 0.8750\n",
      "Epoch 00056: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3798 - accuracy: 0.9167 - val_loss: 0.3262 - val_accuracy: 0.9333\n",
      "Epoch 57/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4563 - accuracy: 0.9375\n",
      "Epoch 00057: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3745 - accuracy: 0.9250 - val_loss: 0.3211 - val_accuracy: 0.9333\n",
      "Epoch 58/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4398 - accuracy: 0.9375\n",
      "Epoch 00058: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3694 - accuracy: 0.9250 - val_loss: 0.3159 - val_accuracy: 0.9333\n",
      "Epoch 59/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3248 - accuracy: 1.0000\n",
      "Epoch 00059: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3638 - accuracy: 0.9250 - val_loss: 0.3109 - val_accuracy: 0.9333\n",
      "Epoch 60/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2642 - accuracy: 0.9375\n",
      "Epoch 00060: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3584 - accuracy: 0.9250 - val_loss: 0.3063 - val_accuracy: 0.9333\n",
      "Epoch 61/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4129 - accuracy: 0.8750\n",
      "Epoch 00061: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3539 - accuracy: 0.9333 - val_loss: 0.3017 - val_accuracy: 0.9333\n",
      "Epoch 62/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3906 - accuracy: 0.9375\n",
      "Epoch 00062: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3488 - accuracy: 0.9333 - val_loss: 0.2973 - val_accuracy: 0.9333\n",
      "Epoch 63/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2248 - accuracy: 0.9375\n",
      "Epoch 00063: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3442 - accuracy: 0.9333 - val_loss: 0.2928 - val_accuracy: 0.9333\n",
      "Epoch 64/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4106 - accuracy: 0.8750\n",
      "Epoch 00064: val_accuracy did not improve from 0.93333\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3397 - accuracy: 0.9333 - val_loss: 0.2883 - val_accuracy: 0.9333\n",
      "Epoch 65/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4724 - accuracy: 0.8750\n",
      "Epoch 00065: val_accuracy improved from 0.93333 to 0.96667, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.3352 - accuracy: 0.9333 - val_loss: 0.2842 - val_accuracy: 0.9667\n",
      "Epoch 66/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2984 - accuracy: 0.9375\n",
      "Epoch 00066: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3309 - accuracy: 0.9333 - val_loss: 0.2801 - val_accuracy: 0.9667\n",
      "Epoch 67/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3089 - accuracy: 0.8125\n",
      "Epoch 00067: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3265 - accuracy: 0.9333 - val_loss: 0.2759 - val_accuracy: 0.9667\n",
      "Epoch 68/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3392 - accuracy: 0.9375\n",
      "Epoch 00068: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3222 - accuracy: 0.9333 - val_loss: 0.2721 - val_accuracy: 0.9667\n",
      "Epoch 69/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3659 - accuracy: 0.8750\n",
      "Epoch 00069: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.9333 - val_loss: 0.2681 - val_accuracy: 0.9667\n",
      "Epoch 70/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3656 - accuracy: 0.9375\n",
      "Epoch 00070: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3140 - accuracy: 0.9333 - val_loss: 0.2643 - val_accuracy: 0.9667\n",
      "Epoch 71/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2928 - accuracy: 0.9375\n",
      "Epoch 00071: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3100 - accuracy: 0.9417 - val_loss: 0.2605 - val_accuracy: 0.9667\n",
      "Epoch 72/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4264 - accuracy: 0.8125\n",
      "Epoch 00072: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3062 - accuracy: 0.9417 - val_loss: 0.2567 - val_accuracy: 0.9667\n",
      "Epoch 73/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2872 - accuracy: 0.8750\n",
      "Epoch 00073: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.3023 - accuracy: 0.9417 - val_loss: 0.2531 - val_accuracy: 0.9667\n",
      "Epoch 74/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2811 - accuracy: 0.9375\n",
      "Epoch 00074: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2986 - accuracy: 0.9417 - val_loss: 0.2495 - val_accuracy: 0.9667\n",
      "Epoch 75/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3256 - accuracy: 0.9375\n",
      "Epoch 00075: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2949 - accuracy: 0.9417 - val_loss: 0.2459 - val_accuracy: 0.9667\n",
      "Epoch 76/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2593 - accuracy: 1.0000\n",
      "Epoch 00076: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2914 - accuracy: 0.9417 - val_loss: 0.2425 - val_accuracy: 0.9667\n",
      "Epoch 77/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1649 - accuracy: 1.0000\n",
      "Epoch 00077: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2880 - accuracy: 0.9417 - val_loss: 0.2391 - val_accuracy: 0.9667\n",
      "Epoch 78/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3282 - accuracy: 0.8750\n",
      "Epoch 00078: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.2845 - accuracy: 0.9417 - val_loss: 0.2357 - val_accuracy: 0.9667\n",
      "Epoch 79/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2779 - accuracy: 0.8750\n",
      "Epoch 00079: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2810 - accuracy: 0.9417 - val_loss: 0.2324 - val_accuracy: 0.9667\n",
      "Epoch 80/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2832 - accuracy: 1.0000\n",
      "Epoch 00080: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2777 - accuracy: 0.9500 - val_loss: 0.2291 - val_accuracy: 0.9667\n",
      "Epoch 81/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.4177 - accuracy: 0.7500\n",
      "Epoch 00081: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2745 - accuracy: 0.9500 - val_loss: 0.2259 - val_accuracy: 0.9667\n",
      "Epoch 82/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1691 - accuracy: 1.0000\n",
      "Epoch 00082: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2711 - accuracy: 0.9500 - val_loss: 0.2228 - val_accuracy: 0.9667\n",
      "Epoch 83/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2265 - accuracy: 1.0000\n",
      "Epoch 00083: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2682 - accuracy: 0.9417 - val_loss: 0.2197 - val_accuracy: 0.9667\n",
      "Epoch 84/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3092 - accuracy: 0.9375\n",
      "Epoch 00084: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2649 - accuracy: 0.9500 - val_loss: 0.2167 - val_accuracy: 0.9667\n",
      "Epoch 85/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2057 - accuracy: 1.0000\n",
      "Epoch 00085: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2619 - accuracy: 0.9500 - val_loss: 0.2139 - val_accuracy: 0.9667\n",
      "Epoch 86/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2744 - accuracy: 0.9375\n",
      "Epoch 00086: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2590 - accuracy: 0.9500 - val_loss: 0.2111 - val_accuracy: 0.9667\n",
      "Epoch 87/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2627 - accuracy: 0.9375\n",
      "Epoch 00087: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2560 - accuracy: 0.9583 - val_loss: 0.2083 - val_accuracy: 0.9667\n",
      "Epoch 88/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1982 - accuracy: 1.0000\n",
      "Epoch 00088: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2532 - accuracy: 0.9583 - val_loss: 0.2057 - val_accuracy: 0.9667\n",
      "Epoch 89/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2590 - accuracy: 0.9375\n",
      "Epoch 00089: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2504 - accuracy: 0.9583 - val_loss: 0.2030 - val_accuracy: 0.9667\n",
      "Epoch 90/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3095 - accuracy: 0.9375\n",
      "Epoch 00090: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2476 - accuracy: 0.9583 - val_loss: 0.2003 - val_accuracy: 0.9667\n",
      "Epoch 91/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2177 - accuracy: 1.0000\n",
      "Epoch 00091: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2449 - accuracy: 0.9583 - val_loss: 0.1977 - val_accuracy: 0.9667\n",
      "Epoch 92/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3077 - accuracy: 0.8750\n",
      "Epoch 00092: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2419 - accuracy: 0.9583 - val_loss: 0.1956 - val_accuracy: 0.9667\n",
      "Epoch 93/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3142 - accuracy: 0.9375\n",
      "Epoch 00093: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2394 - accuracy: 0.9583 - val_loss: 0.1931 - val_accuracy: 0.9667\n",
      "Epoch 94/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2597 - accuracy: 0.8750\n",
      "Epoch 00094: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2369 - accuracy: 0.9583 - val_loss: 0.1908 - val_accuracy: 0.9667\n",
      "Epoch 95/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1703 - accuracy: 1.0000\n",
      "Epoch 00095: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2345 - accuracy: 0.9583 - val_loss: 0.1885 - val_accuracy: 0.9667\n",
      "Epoch 96/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2306 - accuracy: 0.9375\n",
      "Epoch 00096: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2321 - accuracy: 0.9583 - val_loss: 0.1861 - val_accuracy: 0.9667\n",
      "Epoch 97/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2566 - accuracy: 1.0000\n",
      "Epoch 00097: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2298 - accuracy: 0.9583 - val_loss: 0.1836 - val_accuracy: 0.9667\n",
      "Epoch 98/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1656 - accuracy: 1.0000\n",
      "Epoch 00098: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2276 - accuracy: 0.9583 - val_loss: 0.1818 - val_accuracy: 0.9667\n",
      "Epoch 99/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1868 - accuracy: 0.9375\n",
      "Epoch 00099: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2251 - accuracy: 0.9583 - val_loss: 0.1797 - val_accuracy: 0.9667\n",
      "Epoch 100/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2631 - accuracy: 0.9375\n",
      "Epoch 00100: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2228 - accuracy: 0.9583 - val_loss: 0.1776 - val_accuracy: 0.9667\n",
      "Epoch 101/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2820 - accuracy: 0.8750\n",
      "Epoch 00101: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2208 - accuracy: 0.9583 - val_loss: 0.1754 - val_accuracy: 0.9667\n",
      "Epoch 102/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1562 - accuracy: 1.0000\n",
      "Epoch 00102: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2184 - accuracy: 0.9583 - val_loss: 0.1735 - val_accuracy: 0.9667\n",
      "Epoch 103/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3111 - accuracy: 0.8750\n",
      "Epoch 00103: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2164 - accuracy: 0.9583 - val_loss: 0.1714 - val_accuracy: 0.9667\n",
      "Epoch 104/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2620 - accuracy: 0.9375\n",
      "Epoch 00104: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2139 - accuracy: 0.9583 - val_loss: 0.1696 - val_accuracy: 0.9667\n",
      "Epoch 105/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1797 - accuracy: 0.9375\n",
      "Epoch 00105: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2119 - accuracy: 0.9583 - val_loss: 0.1676 - val_accuracy: 0.9667\n",
      "Epoch 106/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1010 - accuracy: 1.0000\n",
      "Epoch 00106: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2098 - accuracy: 0.9500 - val_loss: 0.1656 - val_accuracy: 0.9667\n",
      "Epoch 107/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2168 - accuracy: 0.9375\n",
      "Epoch 00107: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2077 - accuracy: 0.9583 - val_loss: 0.1642 - val_accuracy: 0.9667\n",
      "Epoch 108/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1683 - accuracy: 1.0000\n",
      "Epoch 00108: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2058 - accuracy: 0.9583 - val_loss: 0.1623 - val_accuracy: 0.9667\n",
      "Epoch 109/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.3306 - accuracy: 0.8750\n",
      "Epoch 00109: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2037 - accuracy: 0.9583 - val_loss: 0.1603 - val_accuracy: 0.9667\n",
      "Epoch 110/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2666 - accuracy: 0.8750\n",
      "Epoch 00110: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.2021 - accuracy: 0.9500 - val_loss: 0.1579 - val_accuracy: 0.9667\n",
      "Epoch 111/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1362 - accuracy: 1.0000\n",
      "Epoch 00111: val_accuracy did not improve from 0.96667\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1998 - accuracy: 0.9500 - val_loss: 0.1561 - val_accuracy: 0.9667\n",
      "Epoch 112/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2535 - accuracy: 0.8750\n",
      "Epoch 00112: val_accuracy improved from 0.96667 to 1.00000, saving model to .\\modelo_mlp_ex3_1.hdf5\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.1982 - accuracy: 0.9583 - val_loss: 0.1543 - val_accuracy: 1.0000\n",
      "Epoch 113/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2415 - accuracy: 0.9375\n",
      "Epoch 00113: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1962 - accuracy: 0.9583 - val_loss: 0.1529 - val_accuracy: 1.0000\n",
      "Epoch 114/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1112 - accuracy: 1.0000\n",
      "Epoch 00114: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1943 - accuracy: 0.9583 - val_loss: 0.1510 - val_accuracy: 1.0000\n",
      "Epoch 115/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1614 - accuracy: 1.0000\n",
      "Epoch 00115: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1924 - accuracy: 0.9583 - val_loss: 0.1493 - val_accuracy: 1.0000\n",
      "Epoch 116/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2368 - accuracy: 0.9375\n",
      "Epoch 00116: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1907 - accuracy: 0.9583 - val_loss: 0.1479 - val_accuracy: 1.0000\n",
      "Epoch 117/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1790 - accuracy: 1.0000\n",
      "Epoch 00117: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1889 - accuracy: 0.9583 - val_loss: 0.1463 - val_accuracy: 1.0000\n",
      "Epoch 118/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0841 - accuracy: 1.0000\n",
      "Epoch 00118: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1871 - accuracy: 0.9583 - val_loss: 0.1451 - val_accuracy: 1.0000\n",
      "Epoch 119/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1497 - accuracy: 1.0000\n",
      "Epoch 00119: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1854 - accuracy: 0.9583 - val_loss: 0.1435 - val_accuracy: 1.0000\n",
      "Epoch 120/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1093 - accuracy: 1.0000\n",
      "Epoch 00120: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1838 - accuracy: 0.9583 - val_loss: 0.1417 - val_accuracy: 1.0000\n",
      "Epoch 121/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1532 - accuracy: 0.9375\n",
      "Epoch 00121: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1821 - accuracy: 0.9583 - val_loss: 0.1403 - val_accuracy: 1.0000\n",
      "Epoch 122/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1603 - accuracy: 1.0000\n",
      "Epoch 00122: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1805 - accuracy: 0.9583 - val_loss: 0.1386 - val_accuracy: 1.0000\n",
      "Epoch 123/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1773 - accuracy: 1.0000\n",
      "Epoch 00123: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1789 - accuracy: 0.9583 - val_loss: 0.1372 - val_accuracy: 1.0000\n",
      "Epoch 124/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1711 - accuracy: 1.0000\n",
      "Epoch 00124: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1773 - accuracy: 0.9583 - val_loss: 0.1357 - val_accuracy: 1.0000\n",
      "Epoch 125/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2064 - accuracy: 1.0000\n",
      "Epoch 00125: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1761 - accuracy: 0.9583 - val_loss: 0.1348 - val_accuracy: 1.0000\n",
      "Epoch 126/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1264 - accuracy: 1.0000\n",
      "Epoch 00126: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1740 - accuracy: 0.9583 - val_loss: 0.1334 - val_accuracy: 1.0000\n",
      "Epoch 127/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1631 - accuracy: 1.0000\n",
      "Epoch 00127: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1723 - accuracy: 0.9583 - val_loss: 0.1313 - val_accuracy: 1.0000\n",
      "Epoch 128/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0718 - accuracy: 1.0000\n",
      "Epoch 00128: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9583 - val_loss: 0.1296 - val_accuracy: 1.0000\n",
      "Epoch 129/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2934 - accuracy: 0.8750\n",
      "Epoch 00129: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1696 - accuracy: 0.9583 - val_loss: 0.1284 - val_accuracy: 1.0000\n",
      "Epoch 130/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2214 - accuracy: 0.9375\n",
      "Epoch 00130: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1679 - accuracy: 0.9583 - val_loss: 0.1271 - val_accuracy: 1.0000\n",
      "Epoch 131/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1169 - accuracy: 1.0000\n",
      "Epoch 00131: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1665 - accuracy: 0.9583 - val_loss: 0.1259 - val_accuracy: 1.0000\n",
      "Epoch 132/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0879 - accuracy: 1.0000\n",
      "Epoch 00132: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1650 - accuracy: 0.9583 - val_loss: 0.1248 - val_accuracy: 1.0000\n",
      "Epoch 133/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1537 - accuracy: 0.9375\n",
      "Epoch 00133: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1636 - accuracy: 0.9583 - val_loss: 0.1235 - val_accuracy: 1.0000\n",
      "Epoch 134/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2336 - accuracy: 0.9375\n",
      "Epoch 00134: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1623 - accuracy: 0.9583 - val_loss: 0.1223 - val_accuracy: 1.0000\n",
      "Epoch 135/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1454 - accuracy: 1.0000\n",
      "Epoch 00135: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9583 - val_loss: 0.1213 - val_accuracy: 1.0000\n",
      "Epoch 136/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1824 - accuracy: 1.0000\n",
      "Epoch 00136: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1593 - accuracy: 0.9583 - val_loss: 0.1205 - val_accuracy: 1.0000\n",
      "Epoch 137/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1456 - accuracy: 0.9375\n",
      "Epoch 00137: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1580 - accuracy: 0.9667 - val_loss: 0.1198 - val_accuracy: 1.0000\n",
      "Epoch 138/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1075 - accuracy: 1.0000\n",
      "Epoch 00138: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1565 - accuracy: 0.9667 - val_loss: 0.1189 - val_accuracy: 1.0000\n",
      "Epoch 139/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1929 - accuracy: 0.9375\n",
      "Epoch 00139: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1553 - accuracy: 0.9667 - val_loss: 0.1179 - val_accuracy: 1.0000\n",
      "Epoch 140/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1478 - accuracy: 1.0000\n",
      "Epoch 00140: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1539 - accuracy: 0.9667 - val_loss: 0.1167 - val_accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0734 - accuracy: 1.0000\n",
      "Epoch 00141: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9667 - val_loss: 0.1156 - val_accuracy: 1.0000\n",
      "Epoch 142/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1693 - accuracy: 1.0000\n",
      "Epoch 00142: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1518 - accuracy: 0.9667 - val_loss: 0.1154 - val_accuracy: 1.0000\n",
      "Epoch 143/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1511 - accuracy: 1.0000\n",
      "Epoch 00143: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1502 - accuracy: 0.9667 - val_loss: 0.1140 - val_accuracy: 1.0000\n",
      "Epoch 144/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1778 - accuracy: 0.9375\n",
      "Epoch 00144: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1490 - accuracy: 0.9667 - val_loss: 0.1136 - val_accuracy: 1.0000\n",
      "Epoch 145/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0944 - accuracy: 1.0000\n",
      "Epoch 00145: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1478 - accuracy: 0.9667 - val_loss: 0.1127 - val_accuracy: 1.0000\n",
      "Epoch 146/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1457 - accuracy: 0.9375\n",
      "Epoch 00146: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1467 - accuracy: 0.9667 - val_loss: 0.1114 - val_accuracy: 1.0000\n",
      "Epoch 147/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0986 - accuracy: 1.0000\n",
      "Epoch 00147: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1458 - accuracy: 0.9667 - val_loss: 0.1100 - val_accuracy: 1.0000\n",
      "Epoch 148/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0486 - accuracy: 1.0000\n",
      "Epoch 00148: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9667 - val_loss: 0.1100 - val_accuracy: 1.0000\n",
      "Epoch 149/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1553 - accuracy: 1.0000\n",
      "Epoch 00149: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1433 - accuracy: 0.9667 - val_loss: 0.1091 - val_accuracy: 1.0000\n",
      "Epoch 150/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1548 - accuracy: 1.0000\n",
      "Epoch 00150: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1422 - accuracy: 0.9667 - val_loss: 0.1085 - val_accuracy: 1.0000\n",
      "Epoch 151/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0800 - accuracy: 1.0000\n",
      "Epoch 00151: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9667 - val_loss: 0.1076 - val_accuracy: 1.0000\n",
      "Epoch 152/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1799 - accuracy: 1.0000\n",
      "Epoch 00152: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1401 - accuracy: 0.9667 - val_loss: 0.1069 - val_accuracy: 1.0000\n",
      "Epoch 153/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0754 - accuracy: 1.0000\n",
      "Epoch 00153: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1392 - accuracy: 0.9667 - val_loss: 0.1066 - val_accuracy: 1.0000\n",
      "Epoch 154/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0864 - accuracy: 1.0000\n",
      "Epoch 00154: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1381 - accuracy: 0.9667 - val_loss: 0.1058 - val_accuracy: 1.0000\n",
      "Epoch 155/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0861 - accuracy: 1.0000\n",
      "Epoch 00155: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1372 - accuracy: 0.9667 - val_loss: 0.1049 - val_accuracy: 1.0000\n",
      "Epoch 156/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1475 - accuracy: 0.9375\n",
      "Epoch 00156: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1361 - accuracy: 0.9667 - val_loss: 0.1041 - val_accuracy: 1.0000\n",
      "Epoch 157/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1626 - accuracy: 0.9375\n",
      "Epoch 00157: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1352 - accuracy: 0.9667 - val_loss: 0.1031 - val_accuracy: 1.0000\n",
      "Epoch 158/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1171 - accuracy: 1.0000\n",
      "Epoch 00158: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1342 - accuracy: 0.9667 - val_loss: 0.1022 - val_accuracy: 1.0000\n",
      "Epoch 159/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0751 - accuracy: 1.0000\n",
      "Epoch 00159: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1333 - accuracy: 0.9667 - val_loss: 0.1012 - val_accuracy: 1.0000\n",
      "Epoch 160/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0917 - accuracy: 1.0000\n",
      "Epoch 00160: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1324 - accuracy: 0.9667 - val_loss: 0.1006 - val_accuracy: 1.0000\n",
      "Epoch 161/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1356 - accuracy: 1.0000\n",
      "Epoch 00161: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1315 - accuracy: 0.9667 - val_loss: 0.1001 - val_accuracy: 1.0000\n",
      "Epoch 162/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0718 - accuracy: 1.0000\n",
      "Epoch 00162: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1306 - accuracy: 0.9667 - val_loss: 0.0993 - val_accuracy: 1.0000\n",
      "Epoch 163/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1739 - accuracy: 0.9375\n",
      "Epoch 00163: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9667 - val_loss: 0.0986 - val_accuracy: 1.0000\n",
      "Epoch 164/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1619 - accuracy: 0.9375\n",
      "Epoch 00164: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1290 - accuracy: 0.9667 - val_loss: 0.0980 - val_accuracy: 1.0000\n",
      "Epoch 165/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2241 - accuracy: 0.8750\n",
      "Epoch 00165: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9667 - val_loss: 0.0972 - val_accuracy: 1.0000\n",
      "Epoch 166/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0675 - accuracy: 1.0000\n",
      "Epoch 00166: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1271 - accuracy: 0.9667 - val_loss: 0.0968 - val_accuracy: 1.0000\n",
      "Epoch 167/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1233 - accuracy: 1.0000\n",
      "Epoch 00167: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1263 - accuracy: 0.9667 - val_loss: 0.0960 - val_accuracy: 1.0000\n",
      "Epoch 168/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1221 - accuracy: 1.0000\n",
      "Epoch 00168: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1255 - accuracy: 0.9667 - val_loss: 0.0953 - val_accuracy: 1.0000\n",
      "Epoch 169/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1491 - accuracy: 0.9375\n",
      "Epoch 00169: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1245 - accuracy: 0.9667 - val_loss: 0.0949 - val_accuracy: 1.0000\n",
      "Epoch 170/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2660 - accuracy: 0.8125\n",
      "Epoch 00170: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1239 - accuracy: 0.9667 - val_loss: 0.0948 - val_accuracy: 1.0000\n",
      "Epoch 171/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1097 - accuracy: 0.9375\n",
      "Epoch 00171: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1230 - accuracy: 0.9667 - val_loss: 0.0944 - val_accuracy: 1.0000\n",
      "Epoch 172/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1062 - accuracy: 1.0000\n",
      "Epoch 00172: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1225 - accuracy: 0.9667 - val_loss: 0.0942 - val_accuracy: 1.0000\n",
      "Epoch 173/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 00173: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1213 - accuracy: 0.9667 - val_loss: 0.0930 - val_accuracy: 1.0000\n",
      "Epoch 174/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0547 - accuracy: 1.0000\n",
      "Epoch 00174: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1209 - accuracy: 0.9667 - val_loss: 0.0918 - val_accuracy: 1.0000\n",
      "Epoch 175/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0823 - accuracy: 1.0000\n",
      "Epoch 00175: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9667 - val_loss: 0.0912 - val_accuracy: 1.0000\n",
      "Epoch 176/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0575 - accuracy: 1.0000\n",
      "Epoch 00176: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1193 - accuracy: 0.9667 - val_loss: 0.0906 - val_accuracy: 1.0000\n",
      "Epoch 177/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0768 - accuracy: 1.0000\n",
      "Epoch 00177: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1186 - accuracy: 0.9667 - val_loss: 0.0900 - val_accuracy: 1.0000\n",
      "Epoch 178/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1219 - accuracy: 1.0000\n",
      "Epoch 00178: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 0.9667 - val_loss: 0.0895 - val_accuracy: 1.0000\n",
      "Epoch 179/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1118 - accuracy: 0.9375\n",
      "Epoch 00179: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1172 - accuracy: 0.9667 - val_loss: 0.0887 - val_accuracy: 1.0000\n",
      "Epoch 180/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0564 - accuracy: 1.0000\n",
      "Epoch 00180: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1165 - accuracy: 0.9667 - val_loss: 0.0885 - val_accuracy: 1.0000\n",
      "Epoch 181/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1209 - accuracy: 0.9375\n",
      "Epoch 00181: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1157 - accuracy: 0.9667 - val_loss: 0.0882 - val_accuracy: 1.0000\n",
      "Epoch 182/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0650 - accuracy: 1.0000\n",
      "Epoch 00182: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.9667 - val_loss: 0.0884 - val_accuracy: 1.0000\n",
      "Epoch 183/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0815 - accuracy: 1.0000\n",
      "Epoch 00183: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1145 - accuracy: 0.9667 - val_loss: 0.0885 - val_accuracy: 1.0000\n",
      "Epoch 184/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1235 - accuracy: 0.9375\n",
      "Epoch 00184: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1141 - accuracy: 0.9667 - val_loss: 0.0883 - val_accuracy: 1.0000\n",
      "Epoch 185/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0504 - accuracy: 1.0000\n",
      "Epoch 00185: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1132 - accuracy: 0.9667 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
      "Epoch 186/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0705 - accuracy: 1.0000\n",
      "Epoch 00186: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1127 - accuracy: 0.9667 - val_loss: 0.0869 - val_accuracy: 1.0000\n",
      "Epoch 187/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1681 - accuracy: 0.8750\n",
      "Epoch 00187: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1120 - accuracy: 0.9667 - val_loss: 0.0862 - val_accuracy: 1.0000\n",
      "Epoch 188/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0715 - accuracy: 0.9375\n",
      "Epoch 00188: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1113 - accuracy: 0.9667 - val_loss: 0.0860 - val_accuracy: 1.0000\n",
      "Epoch 189/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1734 - accuracy: 0.9375\n",
      "Epoch 00189: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1108 - accuracy: 0.9667 - val_loss: 0.0855 - val_accuracy: 1.0000\n",
      "Epoch 190/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1698 - accuracy: 0.9375\n",
      "Epoch 00190: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1102 - accuracy: 0.9667 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
      "Epoch 191/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1403 - accuracy: 0.9375\n",
      "Epoch 00191: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1095 - accuracy: 0.9667 - val_loss: 0.0849 - val_accuracy: 1.0000\n",
      "Epoch 192/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0796 - accuracy: 1.0000\n",
      "Epoch 00192: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9667 - val_loss: 0.0845 - val_accuracy: 1.0000\n",
      "Epoch 193/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0350 - accuracy: 1.0000\n",
      "Epoch 00193: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9667 - val_loss: 0.0838 - val_accuracy: 1.0000\n",
      "Epoch 194/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0667 - accuracy: 1.0000\n",
      "Epoch 00194: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1078 - accuracy: 0.9667 - val_loss: 0.0833 - val_accuracy: 1.0000\n",
      "Epoch 195/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2384 - accuracy: 0.8750\n",
      "Epoch 00195: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1072 - accuracy: 0.9667 - val_loss: 0.0830 - val_accuracy: 1.0000\n",
      "Epoch 196/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1471 - accuracy: 0.9375\n",
      "Epoch 00196: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1067 - accuracy: 0.9667 - val_loss: 0.0824 - val_accuracy: 1.0000\n",
      "Epoch 197/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0825 - accuracy: 1.0000\n",
      "Epoch 00197: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1061 - accuracy: 0.9667 - val_loss: 0.0814 - val_accuracy: 1.0000\n",
      "Epoch 198/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1557 - accuracy: 0.9375\n",
      "Epoch 00198: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.9667 - val_loss: 0.0809 - val_accuracy: 1.0000\n",
      "Epoch 199/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1110 - accuracy: 0.9375\n",
      "Epoch 00199: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.9667 - val_loss: 0.0809 - val_accuracy: 1.0000\n",
      "Epoch 200/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2231 - accuracy: 0.8750\n",
      "Epoch 00200: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1048 - accuracy: 0.9667 - val_loss: 0.0798 - val_accuracy: 1.0000\n",
      "Epoch 201/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0465 - accuracy: 1.0000\n",
      "Epoch 00201: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1042 - accuracy: 0.9667 - val_loss: 0.0795 - val_accuracy: 1.0000\n",
      "Epoch 202/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0356 - accuracy: 1.0000\n",
      "Epoch 00202: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1038 - accuracy: 0.9667 - val_loss: 0.0791 - val_accuracy: 1.0000\n",
      "Epoch 203/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0669 - accuracy: 1.0000\n",
      "Epoch 00203: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1031 - accuracy: 0.9667 - val_loss: 0.0786 - val_accuracy: 1.0000\n",
      "Epoch 204/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0613 - accuracy: 1.0000\n",
      "Epoch 00204: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1026 - accuracy: 0.9667 - val_loss: 0.0784 - val_accuracy: 1.0000\n",
      "Epoch 205/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0787 - accuracy: 1.0000\n",
      "Epoch 00205: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1023 - accuracy: 0.9667 - val_loss: 0.0776 - val_accuracy: 1.0000\n",
      "Epoch 206/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1755 - accuracy: 0.8750\n",
      "Epoch 00206: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1022 - accuracy: 0.9667 - val_loss: 0.0780 - val_accuracy: 1.0000\n",
      "Epoch 207/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1575 - accuracy: 1.0000\n",
      "Epoch 00207: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1012 - accuracy: 0.9667 - val_loss: 0.0770 - val_accuracy: 1.0000\n",
      "Epoch 208/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0599 - accuracy: 1.0000\n",
      "Epoch 00208: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1007 - accuracy: 0.9667 - val_loss: 0.0767 - val_accuracy: 1.0000\n",
      "Epoch 209/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2082 - accuracy: 0.8750\n",
      "Epoch 00209: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1003 - accuracy: 0.9667 - val_loss: 0.0761 - val_accuracy: 1.0000\n",
      "Epoch 210/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1865 - accuracy: 0.8750\n",
      "Epoch 00210: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0999 - accuracy: 0.9667 - val_loss: 0.0759 - val_accuracy: 1.0000\n",
      "Epoch 211/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0370 - accuracy: 1.0000\n",
      "Epoch 00211: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0994 - accuracy: 0.9667 - val_loss: 0.0755 - val_accuracy: 1.0000\n",
      "Epoch 212/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0555 - accuracy: 1.0000\n",
      "Epoch 00212: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0990 - accuracy: 0.9667 - val_loss: 0.0753 - val_accuracy: 1.0000\n",
      "Epoch 213/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0832 - accuracy: 0.9375\n",
      "Epoch 00213: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0987 - accuracy: 0.9667 - val_loss: 0.0757 - val_accuracy: 1.0000\n",
      "Epoch 214/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1230 - accuracy: 0.9375\n",
      "Epoch 00214: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0981 - accuracy: 0.9667 - val_loss: 0.0753 - val_accuracy: 1.0000\n",
      "Epoch 215/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0519 - accuracy: 1.0000\n",
      "Epoch 00215: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9667 - val_loss: 0.0751 - val_accuracy: 1.0000\n",
      "Epoch 216/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0641 - accuracy: 1.0000\n",
      "Epoch 00216: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0973 - accuracy: 0.9667 - val_loss: 0.0746 - val_accuracy: 1.0000\n",
      "Epoch 217/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0790 - accuracy: 1.0000\n",
      "Epoch 00217: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0967 - accuracy: 0.9667 - val_loss: 0.0737 - val_accuracy: 1.0000\n",
      "Epoch 218/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 00218: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9667 - val_loss: 0.0734 - val_accuracy: 1.0000\n",
      "Epoch 219/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0424 - accuracy: 1.0000\n",
      "Epoch 00219: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0960 - accuracy: 0.9667 - val_loss: 0.0728 - val_accuracy: 1.0000\n",
      "Epoch 220/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1029 - accuracy: 0.9375\n",
      "Epoch 00220: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9667 - val_loss: 0.0724 - val_accuracy: 1.0000\n",
      "Epoch 221/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1075 - accuracy: 0.9375\n",
      "Epoch 00221: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0953 - accuracy: 0.9667 - val_loss: 0.0719 - val_accuracy: 1.0000\n",
      "Epoch 222/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0415 - accuracy: 1.0000\n",
      "Epoch 00222: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0947 - accuracy: 0.9667 - val_loss: 0.0716 - val_accuracy: 1.0000\n",
      "Epoch 223/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1085 - accuracy: 0.9375\n",
      "Epoch 00223: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0943 - accuracy: 0.9667 - val_loss: 0.0712 - val_accuracy: 1.0000\n",
      "Epoch 224/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0273 - accuracy: 1.0000\n",
      "Epoch 00224: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0939 - accuracy: 0.9667 - val_loss: 0.0710 - val_accuracy: 1.0000\n",
      "Epoch 225/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1694 - accuracy: 0.8750\n",
      "Epoch 00225: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0935 - accuracy: 0.9667 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
      "Epoch 226/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0556 - accuracy: 1.0000\n",
      "Epoch 00226: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0932 - accuracy: 0.9667 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
      "Epoch 227/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1568 - accuracy: 0.8750\n",
      "Epoch 00227: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0929 - accuracy: 0.9667 - val_loss: 0.0706 - val_accuracy: 1.0000\n",
      "Epoch 228/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1158 - accuracy: 1.0000\n",
      "Epoch 00228: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0924 - accuracy: 0.9667 - val_loss: 0.0702 - val_accuracy: 1.0000\n",
      "Epoch 229/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0873 - accuracy: 1.0000\n",
      "Epoch 00229: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0921 - accuracy: 0.9667 - val_loss: 0.0698 - val_accuracy: 1.0000\n",
      "Epoch 230/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1631 - accuracy: 0.8750\n",
      "Epoch 00230: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0918 - accuracy: 0.9667 - val_loss: 0.0696 - val_accuracy: 1.0000\n",
      "Epoch 231/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1422 - accuracy: 0.9375\n",
      "Epoch 00231: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0914 - accuracy: 0.9667 - val_loss: 0.0696 - val_accuracy: 1.0000\n",
      "Epoch 232/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0698 - accuracy: 1.0000\n",
      "Epoch 00232: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0910 - accuracy: 0.9667 - val_loss: 0.0694 - val_accuracy: 1.0000\n",
      "Epoch 233/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1231 - accuracy: 0.9375\n",
      "Epoch 00233: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0907 - accuracy: 0.9667 - val_loss: 0.0691 - val_accuracy: 1.0000\n",
      "Epoch 234/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0645 - accuracy: 1.0000\n",
      "Epoch 00234: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0903 - accuracy: 0.9667 - val_loss: 0.0690 - val_accuracy: 1.0000\n",
      "Epoch 235/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1380 - accuracy: 0.9375\n",
      "Epoch 00235: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0900 - accuracy: 0.9667 - val_loss: 0.0689 - val_accuracy: 1.0000\n",
      "Epoch 236/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1322 - accuracy: 0.9375\n",
      "Epoch 00236: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9667 - val_loss: 0.0687 - val_accuracy: 1.0000\n",
      "Epoch 237/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0664 - accuracy: 0.9375\n",
      "Epoch 00237: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0893 - accuracy: 0.9667 - val_loss: 0.0682 - val_accuracy: 1.0000\n",
      "Epoch 238/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0811 - accuracy: 0.9375\n",
      "Epoch 00238: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0889 - accuracy: 0.9667 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
      "Epoch 239/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0434 - accuracy: 1.0000\n",
      "Epoch 00239: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.9667 - val_loss: 0.0678 - val_accuracy: 1.0000\n",
      "Epoch 240/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0663 - accuracy: 1.0000\n",
      "Epoch 00240: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0884 - accuracy: 0.9667 - val_loss: 0.0673 - val_accuracy: 1.0000\n",
      "Epoch 241/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0518 - accuracy: 1.0000\n",
      "Epoch 00241: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0886 - accuracy: 0.9583 - val_loss: 0.0678 - val_accuracy: 1.0000\n",
      "Epoch 242/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1188 - accuracy: 0.9375\n",
      "Epoch 00242: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0876 - accuracy: 0.9583 - val_loss: 0.0673 - val_accuracy: 1.0000\n",
      "Epoch 243/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2004 - accuracy: 0.8125\n",
      "Epoch 00243: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0878 - accuracy: 0.9583 - val_loss: 0.0667 - val_accuracy: 1.0000\n",
      "Epoch 244/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0436 - accuracy: 1.0000\n",
      "Epoch 00244: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9667 - val_loss: 0.0664 - val_accuracy: 1.0000\n",
      "Epoch 245/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0641 - accuracy: 1.0000\n",
      "Epoch 00245: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0869 - accuracy: 0.9667 - val_loss: 0.0667 - val_accuracy: 1.0000\n",
      "Epoch 246/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0527 - accuracy: 1.0000\n",
      "Epoch 00246: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0865 - accuracy: 0.9583 - val_loss: 0.0659 - val_accuracy: 1.0000\n",
      "Epoch 247/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1797 - accuracy: 0.8750\n",
      "Epoch 00247: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0862 - accuracy: 0.9667 - val_loss: 0.0660 - val_accuracy: 1.0000\n",
      "Epoch 248/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0643 - accuracy: 0.9375\n",
      "Epoch 00248: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0858 - accuracy: 0.9667 - val_loss: 0.0656 - val_accuracy: 1.0000\n",
      "Epoch 249/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0570 - accuracy: 1.0000\n",
      "Epoch 00249: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0856 - accuracy: 0.9667 - val_loss: 0.0653 - val_accuracy: 1.0000\n",
      "Epoch 250/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0369 - accuracy: 1.0000\n",
      "Epoch 00250: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0852 - accuracy: 0.9667 - val_loss: 0.0649 - val_accuracy: 1.0000\n",
      "Epoch 251/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1309 - accuracy: 0.9375\n",
      "Epoch 00251: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0851 - accuracy: 0.9667 - val_loss: 0.0643 - val_accuracy: 1.0000\n",
      "Epoch 252/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1337 - accuracy: 0.9375\n",
      "Epoch 00252: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9667 - val_loss: 0.0642 - val_accuracy: 1.0000\n",
      "Epoch 253/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0810 - accuracy: 1.0000\n",
      "Epoch 00253: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9667 - val_loss: 0.0642 - val_accuracy: 1.0000\n",
      "Epoch 254/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0974 - accuracy: 1.0000\n",
      "Epoch 00254: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0844 - accuracy: 0.9667 - val_loss: 0.0644 - val_accuracy: 1.0000\n",
      "Epoch 255/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0207 - accuracy: 1.0000\n",
      "Epoch 00255: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0839 - accuracy: 0.9583 - val_loss: 0.0643 - val_accuracy: 1.0000\n",
      "Epoch 256/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0400 - accuracy: 1.0000\n",
      "Epoch 00256: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0835 - accuracy: 0.9583 - val_loss: 0.0644 - val_accuracy: 1.0000\n",
      "Epoch 257/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0947 - accuracy: 0.9375\n",
      "Epoch 00257: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0834 - accuracy: 0.9583 - val_loss: 0.0642 - val_accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1173 - accuracy: 0.9375\n",
      "Epoch 00258: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0832 - accuracy: 0.9583 - val_loss: 0.0637 - val_accuracy: 1.0000\n",
      "Epoch 259/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1483 - accuracy: 0.8750\n",
      "Epoch 00259: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9583 - val_loss: 0.0632 - val_accuracy: 1.0000\n",
      "Epoch 260/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0763 - accuracy: 0.9375\n",
      "Epoch 00260: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9583 - val_loss: 0.0631 - val_accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1120 - accuracy: 0.9375\n",
      "Epoch 00261: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0830 - accuracy: 0.9667 - val_loss: 0.0625 - val_accuracy: 1.0000\n",
      "Epoch 262/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0357 - accuracy: 1.0000\n",
      "Epoch 00262: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0821 - accuracy: 0.9667 - val_loss: 0.0630 - val_accuracy: 1.0000\n",
      "Epoch 263/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 00263: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.9667 - val_loss: 0.0629 - val_accuracy: 1.0000\n",
      "Epoch 264/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1144 - accuracy: 0.9375\n",
      "Epoch 00264: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.9667 - val_loss: 0.0623 - val_accuracy: 1.0000\n",
      "Epoch 265/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1159 - accuracy: 0.9375\n",
      "Epoch 00265: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0815 - accuracy: 0.9667 - val_loss: 0.0622 - val_accuracy: 1.0000\n",
      "Epoch 266/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1241 - accuracy: 0.9375\n",
      "Epoch 00266: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0821 - accuracy: 0.9583 - val_loss: 0.0630 - val_accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0787 - accuracy: 0.9375\n",
      "Epoch 00267: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9583 - val_loss: 0.0628 - val_accuracy: 1.0000\n",
      "Epoch 268/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1513 - accuracy: 0.8750\n",
      "Epoch 00268: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0805 - accuracy: 0.9583 - val_loss: 0.0625 - val_accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0902 - accuracy: 0.9375\n",
      "Epoch 00269: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9667 - val_loss: 0.0622 - val_accuracy: 1.0000\n",
      "Epoch 270/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0980 - accuracy: 0.9375\n",
      "Epoch 00270: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0804 - accuracy: 0.9667 - val_loss: 0.0620 - val_accuracy: 1.0000\n",
      "Epoch 271/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0967 - accuracy: 0.9375\n",
      "Epoch 00271: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0801 - accuracy: 0.9667 - val_loss: 0.0619 - val_accuracy: 1.0000\n",
      "Epoch 272/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0403 - accuracy: 1.0000\n",
      "Epoch 00272: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0798 - accuracy: 0.9667 - val_loss: 0.0613 - val_accuracy: 1.0000\n",
      "Epoch 273/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 00273: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0796 - accuracy: 0.9667 - val_loss: 0.0610 - val_accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1030 - accuracy: 0.9375\n",
      "Epoch 00274: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0794 - accuracy: 0.9667 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 275/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0877 - accuracy: 1.0000\n",
      "Epoch 00275: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0791 - accuracy: 0.9667 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 276/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0544 - accuracy: 1.0000\n",
      "Epoch 00276: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0789 - accuracy: 0.9667 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1685 - accuracy: 0.8750\n",
      "Epoch 00277: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0786 - accuracy: 0.9583 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
      "Epoch 278/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 00278: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0784 - accuracy: 0.9583 - val_loss: 0.0607 - val_accuracy: 1.0000\n",
      "Epoch 279/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0859 - accuracy: 0.9375\n",
      "Epoch 00279: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0783 - accuracy: 0.9583 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
      "Epoch 280/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0752 - accuracy: 0.9375\n",
      "Epoch 00280: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0781 - accuracy: 0.9667 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
      "Epoch 281/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 00281: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0782 - accuracy: 0.9583 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
      "Epoch 282/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0840 - accuracy: 0.9375\n",
      "Epoch 00282: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0776 - accuracy: 0.9583 - val_loss: 0.0603 - val_accuracy: 1.0000\n",
      "Epoch 283/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1438 - accuracy: 0.8750\n",
      "Epoch 00283: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0775 - accuracy: 0.9583 - val_loss: 0.0601 - val_accuracy: 1.0000\n",
      "Epoch 284/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 00284: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0772 - accuracy: 0.9583 - val_loss: 0.0597 - val_accuracy: 1.0000\n",
      "Epoch 285/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 00285: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0771 - accuracy: 0.9583 - val_loss: 0.0597 - val_accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0655 - accuracy: 0.9375\n",
      "Epoch 00286: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9583 - val_loss: 0.0591 - val_accuracy: 1.0000\n",
      "Epoch 287/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0691 - accuracy: 1.0000\n",
      "Epoch 00287: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.9583 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
      "Epoch 288/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0288 - accuracy: 1.0000\n",
      "Epoch 00288: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0766 - accuracy: 0.9583 - val_loss: 0.0592 - val_accuracy: 1.0000\n",
      "Epoch 289/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0384 - accuracy: 1.0000\n",
      "Epoch 00289: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0763 - accuracy: 0.9583 - val_loss: 0.0589 - val_accuracy: 1.0000\n",
      "Epoch 290/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0695 - accuracy: 0.9375\n",
      "Epoch 00290: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0765 - accuracy: 0.9583 - val_loss: 0.0581 - val_accuracy: 1.0000\n",
      "Epoch 291/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0677 - accuracy: 1.0000\n",
      "Epoch 00291: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0767 - accuracy: 0.9583 - val_loss: 0.0584 - val_accuracy: 1.0000\n",
      "Epoch 292/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0987 - accuracy: 0.9375\n",
      "Epoch 00292: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.9583 - val_loss: 0.0584 - val_accuracy: 1.0000\n",
      "Epoch 293/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0900 - accuracy: 0.9375\n",
      "Epoch 00293: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0758 - accuracy: 0.9583 - val_loss: 0.0589 - val_accuracy: 1.0000\n",
      "Epoch 294/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1388 - accuracy: 0.9375\n",
      "Epoch 00294: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0756 - accuracy: 0.9583 - val_loss: 0.0588 - val_accuracy: 1.0000\n",
      "Epoch 295/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 00295: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0753 - accuracy: 0.9583 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
      "Epoch 296/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0857 - accuracy: 0.9375\n",
      "Epoch 00296: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0752 - accuracy: 0.9583 - val_loss: 0.0583 - val_accuracy: 1.0000\n",
      "Epoch 297/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0528 - accuracy: 1.0000\n",
      "Epoch 00297: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0749 - accuracy: 0.9583 - val_loss: 0.0575 - val_accuracy: 1.0000\n",
      "Epoch 298/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0791 - accuracy: 0.9375\n",
      "Epoch 00298: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0748 - accuracy: 0.9667 - val_loss: 0.0572 - val_accuracy: 1.0000\n",
      "Epoch 299/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0351 - accuracy: 1.0000\n",
      "Epoch 00299: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0747 - accuracy: 0.9667 - val_loss: 0.0569 - val_accuracy: 1.0000\n",
      "Epoch 300/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 00300: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0745 - accuracy: 0.9667 - val_loss: 0.0567 - val_accuracy: 1.0000\n",
      "Epoch 301/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1071 - accuracy: 1.0000\n",
      "Epoch 00301: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0744 - accuracy: 0.9667 - val_loss: 0.0569 - val_accuracy: 1.0000\n",
      "Epoch 302/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0534 - accuracy: 1.0000\n",
      "Epoch 00302: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9583 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
      "Epoch 303/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0577 - accuracy: 1.0000\n",
      "Epoch 00303: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0743 - accuracy: 0.9583 - val_loss: 0.0582 - val_accuracy: 0.9667\n",
      "Epoch 304/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 00304: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0738 - accuracy: 0.9583 - val_loss: 0.0579 - val_accuracy: 1.0000\n",
      "Epoch 305/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0619 - accuracy: 0.9375\n",
      "Epoch 00305: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0737 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 306/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1015 - accuracy: 0.9375\n",
      "Epoch 00306: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0734 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 307/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.2346 - accuracy: 0.8125\n",
      "Epoch 00307: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0733 - accuracy: 0.9583 - val_loss: 0.0578 - val_accuracy: 1.0000\n",
      "Epoch 308/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 00308: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9583 - val_loss: 0.0577 - val_accuracy: 1.0000\n",
      "Epoch 309/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1068 - accuracy: 0.9375\n",
      "Epoch 00309: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0729 - accuracy: 0.9583 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
      "Epoch 310/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1014 - accuracy: 0.9375\n",
      "Epoch 00310: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0731 - accuracy: 0.9583 - val_loss: 0.0572 - val_accuracy: 1.0000\n",
      "Epoch 311/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0654 - accuracy: 0.9375\n",
      "Epoch 00311: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0728 - accuracy: 0.9583 - val_loss: 0.0575 - val_accuracy: 1.0000\n",
      "Epoch 312/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1205 - accuracy: 0.9375\n",
      "Epoch 00312: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0725 - accuracy: 0.9583 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
      "Epoch 313/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 00313: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0726 - accuracy: 0.9583 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
      "Epoch 314/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1727 - accuracy: 0.8750\n",
      "Epoch 00314: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0722 - accuracy: 0.9583 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
      "Epoch 315/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 00315: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0722 - accuracy: 0.9583 - val_loss: 0.0574 - val_accuracy: 0.9667\n",
      "Epoch 316/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 00316: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0719 - accuracy: 0.9583 - val_loss: 0.0572 - val_accuracy: 0.9667\n",
      "Epoch 317/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1039 - accuracy: 0.9375\n",
      "Epoch 00317: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0718 - accuracy: 0.9583 - val_loss: 0.0572 - val_accuracy: 0.9667\n",
      "Epoch 318/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0459 - accuracy: 1.0000\n",
      "Epoch 00318: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0717 - accuracy: 0.9583 - val_loss: 0.0571 - val_accuracy: 0.9667\n",
      "Epoch 319/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 00319: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9583 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 320/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0622 - accuracy: 1.0000\n",
      "Epoch 00320: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0713 - accuracy: 0.9583 - val_loss: 0.0569 - val_accuracy: 0.9667\n",
      "Epoch 321/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0990 - accuracy: 0.9375\n",
      "Epoch 00321: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0715 - accuracy: 0.9583 - val_loss: 0.0570 - val_accuracy: 0.9667\n",
      "Epoch 322/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0934 - accuracy: 0.9375\n",
      "Epoch 00322: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0710 - accuracy: 0.9583 - val_loss: 0.0567 - val_accuracy: 0.9667\n",
      "Epoch 323/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0230 - accuracy: 1.0000\n",
      "Epoch 00323: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9583 - val_loss: 0.0565 - val_accuracy: 1.0000\n",
      "Epoch 324/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0630 - accuracy: 0.9375\n",
      "Epoch 00324: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9583 - val_loss: 0.0564 - val_accuracy: 1.0000\n",
      "Epoch 325/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1419 - accuracy: 0.9375\n",
      "Epoch 00325: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9583 - val_loss: 0.0560 - val_accuracy: 1.0000\n",
      "Epoch 326/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 00326: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0706 - accuracy: 0.9583 - val_loss: 0.0556 - val_accuracy: 1.0000\n",
      "Epoch 327/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0532 - accuracy: 1.0000\n",
      "Epoch 00327: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0704 - accuracy: 0.9583 - val_loss: 0.0557 - val_accuracy: 1.0000\n",
      "Epoch 328/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0968 - accuracy: 0.9375\n",
      "Epoch 00328: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0704 - accuracy: 0.9583 - val_loss: 0.0561 - val_accuracy: 0.9667\n",
      "Epoch 329/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 00329: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.9583 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 330/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0975 - accuracy: 0.9375\n",
      "Epoch 00330: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9583 - val_loss: 0.0558 - val_accuracy: 0.9667\n",
      "Epoch 331/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 00331: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0698 - accuracy: 0.9583 - val_loss: 0.0560 - val_accuracy: 0.9667\n",
      "Epoch 332/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0550 - accuracy: 1.0000\n",
      "Epoch 00332: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0698 - accuracy: 0.9583 - val_loss: 0.0563 - val_accuracy: 0.9667\n",
      "Epoch 333/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0501 - accuracy: 1.0000\n",
      "Epoch 00333: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0696 - accuracy: 0.9583 - val_loss: 0.0558 - val_accuracy: 0.9667\n",
      "Epoch 334/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0810 - accuracy: 0.9375\n",
      "Epoch 00334: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0694 - accuracy: 0.9583 - val_loss: 0.0551 - val_accuracy: 1.0000\n",
      "Epoch 335/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 00335: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.9583 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
      "Epoch 336/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0452 - accuracy: 1.0000\n",
      "Epoch 00336: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0693 - accuracy: 0.9583 - val_loss: 0.0549 - val_accuracy: 0.9667\n",
      "Epoch 337/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000\n",
      "Epoch 00337: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 0.9583 - val_loss: 0.0542 - val_accuracy: 1.0000\n",
      "Epoch 338/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 00338: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9583 - val_loss: 0.0537 - val_accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1396 - accuracy: 0.8750\n",
      "Epoch 00339: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0691 - accuracy: 0.9583 - val_loss: 0.0533 - val_accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0344 - accuracy: 1.0000\n",
      "Epoch 00340: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0688 - accuracy: 0.9583 - val_loss: 0.0534 - val_accuracy: 1.0000\n",
      "Epoch 341/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0866 - accuracy: 0.9375\n",
      "Epoch 00341: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0688 - accuracy: 0.9583 - val_loss: 0.0532 - val_accuracy: 1.0000\n",
      "Epoch 342/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0883 - accuracy: 0.9375\n",
      "Epoch 00342: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0685 - accuracy: 0.9583 - val_loss: 0.0533 - val_accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1833 - accuracy: 0.8750\n",
      "Epoch 00343: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0687 - accuracy: 0.9583 - val_loss: 0.0532 - val_accuracy: 1.0000\n",
      "Epoch 344/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0808 - accuracy: 0.9375\n",
      "Epoch 00344: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.9583 - val_loss: 0.0531 - val_accuracy: 1.0000\n",
      "Epoch 345/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0369 - accuracy: 1.0000\n",
      "Epoch 00345: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0683 - accuracy: 0.9583 - val_loss: 0.0530 - val_accuracy: 1.0000\n",
      "Epoch 346/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0615 - accuracy: 0.9375\n",
      "Epoch 00346: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0680 - accuracy: 0.9583 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0338 - accuracy: 1.0000\n",
      "Epoch 00347: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0681 - accuracy: 0.9583 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
      "Epoch 348/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1015 - accuracy: 0.9375\n",
      "Epoch 00348: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0682 - accuracy: 0.9583 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 00349: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.9583 - val_loss: 0.0526 - val_accuracy: 1.0000\n",
      "Epoch 350/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0931 - accuracy: 0.9375\n",
      "Epoch 00350: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0679 - accuracy: 0.9583 - val_loss: 0.0528 - val_accuracy: 0.9667\n",
      "Epoch 351/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0804 - accuracy: 0.9375\n",
      "Epoch 00351: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0677 - accuracy: 0.9583 - val_loss: 0.0527 - val_accuracy: 1.0000\n",
      "Epoch 352/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 00352: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0675 - accuracy: 0.9583 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
      "Epoch 353/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1214 - accuracy: 0.9375\n",
      "Epoch 00353: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9583 - val_loss: 0.0523 - val_accuracy: 1.0000\n",
      "Epoch 354/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0453 - accuracy: 1.0000\n",
      "Epoch 00354: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9583 - val_loss: 0.0526 - val_accuracy: 0.9667\n",
      "Epoch 355/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0635 - accuracy: 0.9375\n",
      "Epoch 00355: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9583 - val_loss: 0.0521 - val_accuracy: 1.0000\n",
      "Epoch 356/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0992 - accuracy: 0.9375\n",
      "Epoch 00356: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.9583 - val_loss: 0.0523 - val_accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 00357: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0669 - accuracy: 0.9583 - val_loss: 0.0523 - val_accuracy: 0.9667\n",
      "Epoch 358/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0738 - accuracy: 0.9375\n",
      "Epoch 00358: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0668 - accuracy: 0.9583 - val_loss: 0.0522 - val_accuracy: 1.0000\n",
      "Epoch 359/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 00359: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0669 - accuracy: 0.9583 - val_loss: 0.0524 - val_accuracy: 0.9667\n",
      "Epoch 360/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 00360: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0668 - accuracy: 0.9583 - val_loss: 0.0514 - val_accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1220 - accuracy: 0.8750\n",
      "Epoch 00361: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9583 - val_loss: 0.0513 - val_accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1077 - accuracy: 0.8750\n",
      "Epoch 00362: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.9583 - val_loss: 0.0509 - val_accuracy: 1.0000\n",
      "Epoch 363/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0244 - accuracy: 1.0000\n",
      "Epoch 00363: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0663 - accuracy: 0.9583 - val_loss: 0.0510 - val_accuracy: 1.0000\n",
      "Epoch 364/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0457 - accuracy: 1.0000\n",
      "Epoch 00364: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.9583 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
      "Epoch 365/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1000 - accuracy: 0.9375\n",
      "Epoch 00365: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.9583 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
      "Epoch 366/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 00366: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0664 - accuracy: 0.9667 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0297 - accuracy: 1.0000\n",
      "Epoch 00367: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.9667 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0847 - accuracy: 0.9375\n",
      "Epoch 00368: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.9583 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
      "Epoch 369/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 00369: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.9583 - val_loss: 0.0508 - val_accuracy: 1.0000\n",
      "Epoch 370/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0565 - accuracy: 1.0000\n",
      "Epoch 00370: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0660 - accuracy: 0.9583 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
      "Epoch 371/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0574 - accuracy: 0.9375\n",
      "Epoch 00371: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0657 - accuracy: 0.9583 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 00372: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0655 - accuracy: 0.9583 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 373/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 00373: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0661 - accuracy: 0.9583 - val_loss: 0.0506 - val_accuracy: 1.0000\n",
      "Epoch 374/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0320 - accuracy: 1.0000\n",
      "Epoch 00374: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.9583 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 00375: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0659 - accuracy: 0.9750 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
      "Epoch 376/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1755 - accuracy: 0.8750\n",
      "Epoch 00376: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.9750 - val_loss: 0.0494 - val_accuracy: 1.0000\n",
      "Epoch 377/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 00377: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.9750 - val_loss: 0.0493 - val_accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0676 - accuracy: 0.9375\n",
      "Epoch 00378: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0651 - accuracy: 0.9750 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
      "Epoch 379/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0809 - accuracy: 0.9375\n",
      "Epoch 00379: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0649 - accuracy: 0.9750 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 380/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0757 - accuracy: 0.9375\n",
      "Epoch 00380: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0648 - accuracy: 0.9583 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
      "Epoch 381/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0192 - accuracy: 1.0000\n",
      "Epoch 00381: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9583 - val_loss: 0.0504 - val_accuracy: 1.0000\n",
      "Epoch 382/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0457 - accuracy: 1.0000\n",
      "Epoch 00382: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0652 - accuracy: 0.9583 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 383/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 00383: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9583 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1479 - accuracy: 0.8750\n",
      "Epoch 00384: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0644 - accuracy: 0.9583 - val_loss: 0.0503 - val_accuracy: 1.0000\n",
      "Epoch 385/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0234 - accuracy: 1.0000\n",
      "Epoch 00385: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9583 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
      "Epoch 386/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0659 - accuracy: 0.9375\n",
      "Epoch 00386: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0643 - accuracy: 0.9583 - val_loss: 0.0501 - val_accuracy: 1.0000\n",
      "Epoch 387/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 00387: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0642 - accuracy: 0.9583 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 388/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1124 - accuracy: 0.9375\n",
      "Epoch 00388: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.9583 - val_loss: 0.0500 - val_accuracy: 1.0000\n",
      "Epoch 389/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0752 - accuracy: 0.9375\n",
      "Epoch 00389: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0641 - accuracy: 0.9583 - val_loss: 0.0502 - val_accuracy: 1.0000\n",
      "Epoch 390/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0570 - accuracy: 0.9375\n",
      "Epoch 00390: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.9583 - val_loss: 0.0498 - val_accuracy: 1.0000\n",
      "Epoch 391/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0318 - accuracy: 1.0000\n",
      "Epoch 00391: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0646 - accuracy: 0.9583 - val_loss: 0.0502 - val_accuracy: 0.9667\n",
      "Epoch 392/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 00392: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9583 - val_loss: 0.0497 - val_accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0713 - accuracy: 0.9375\n",
      "Epoch 00393: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0638 - accuracy: 0.9667 - val_loss: 0.0494 - val_accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0307 - accuracy: 1.0000\n",
      "Epoch 00394: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9667 - val_loss: 0.0494 - val_accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1302 - accuracy: 0.9375\n",
      "Epoch 00395: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0640 - accuracy: 0.9750 - val_loss: 0.0493 - val_accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0468 - accuracy: 1.0000\n",
      "Epoch 00396: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0637 - accuracy: 0.9667 - val_loss: 0.0497 - val_accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0620 - accuracy: 1.0000\n",
      "Epoch 00397: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0635 - accuracy: 0.9667 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0894 - accuracy: 1.0000\n",
      "Epoch 00398: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9583 - val_loss: 0.0501 - val_accuracy: 0.9667\n",
      "Epoch 399/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1116 - accuracy: 0.9375\n",
      "Epoch 00399: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0633 - accuracy: 0.9583 - val_loss: 0.0500 - val_accuracy: 0.9667\n",
      "Epoch 400/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1431 - accuracy: 0.8750\n",
      "Epoch 00400: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0636 - accuracy: 0.9583 - val_loss: 0.0493 - val_accuracy: 1.0000\n",
      "Epoch 401/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0160 - accuracy: 1.0000\n",
      "Epoch 00401: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9583 - val_loss: 0.0498 - val_accuracy: 0.9667\n",
      "Epoch 402/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0282 - accuracy: 1.0000\n",
      "Epoch 00402: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0636 - accuracy: 0.9583 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 403/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1270 - accuracy: 0.8750\n",
      "Epoch 00403: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0631 - accuracy: 0.9583 - val_loss: 0.0507 - val_accuracy: 0.9667\n",
      "Epoch 404/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 00404: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0632 - accuracy: 0.9583 - val_loss: 0.0506 - val_accuracy: 0.9667\n",
      "Epoch 405/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0475 - accuracy: 1.0000\n",
      "Epoch 00405: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0630 - accuracy: 0.9583 - val_loss: 0.0499 - val_accuracy: 0.9667\n",
      "Epoch 406/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0301 - accuracy: 1.0000\n",
      "Epoch 00406: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0629 - accuracy: 0.9583 - val_loss: 0.0491 - val_accuracy: 1.0000\n",
      "Epoch 407/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0353 - accuracy: 1.0000\n",
      "Epoch 00407: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.9583 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
      "Epoch 408/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 00408: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0627 - accuracy: 0.9583 - val_loss: 0.0490 - val_accuracy: 1.0000\n",
      "Epoch 409/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 00409: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0627 - accuracy: 0.9583 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
      "Epoch 410/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1162 - accuracy: 0.8750\n",
      "Epoch 00410: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0628 - accuracy: 0.9667 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0327 - accuracy: 1.0000\n",
      "Epoch 00411: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.9583 - val_loss: 0.0484 - val_accuracy: 1.0000\n",
      "Epoch 412/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 00412: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.9667 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
      "Epoch 413/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0119 - accuracy: 1.0000\n",
      "Epoch 00413: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.9750 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
      "Epoch 414/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000\n",
      "Epoch 00414: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0625 - accuracy: 0.9750 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
      "Epoch 415/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0577 - accuracy: 1.0000\n",
      "Epoch 00415: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0623 - accuracy: 0.9667 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1933 - accuracy: 0.8750\n",
      "Epoch 00416: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0624 - accuracy: 0.9750 - val_loss: 0.0469 - val_accuracy: 1.0000\n",
      "Epoch 417/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0520 - accuracy: 0.9375\n",
      "Epoch 00417: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9750 - val_loss: 0.0473 - val_accuracy: 1.0000\n",
      "Epoch 418/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 00418: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9667 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0768 - accuracy: 0.9375\n",
      "Epoch 00419: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0619 - accuracy: 0.9667 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 00420: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.9583 - val_loss: 0.0479 - val_accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0291 - accuracy: 1.0000\n",
      "Epoch 00421: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0619 - accuracy: 0.9583 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0662 - accuracy: 0.9375\n",
      "Epoch 00422: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0617 - accuracy: 0.9583 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 00423: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0621 - accuracy: 0.9583 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 424/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0220 - accuracy: 1.0000\n",
      "Epoch 00424: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.9583 - val_loss: 0.0482 - val_accuracy: 0.9667\n",
      "Epoch 425/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0319 - accuracy: 1.0000\n",
      "Epoch 00425: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0618 - accuracy: 0.9583 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 00426: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0616 - accuracy: 0.9667 - val_loss: 0.0478 - val_accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0654 - accuracy: 0.9375\n",
      "Epoch 00427: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.9667 - val_loss: 0.0476 - val_accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 00428: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0614 - accuracy: 0.9667 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0549 - accuracy: 0.9375\n",
      "Epoch 00429: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0613 - accuracy: 0.9667 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1281 - accuracy: 0.9375\n",
      "Epoch 00430: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9583 - val_loss: 0.0481 - val_accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 00431: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9583 - val_loss: 0.0483 - val_accuracy: 0.9667\n",
      "Epoch 432/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1244 - accuracy: 0.9375\n",
      "Epoch 00432: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9667 - val_loss: 0.0480 - val_accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0394 - accuracy: 1.0000\n",
      "Epoch 00433: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9667 - val_loss: 0.0475 - val_accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0950 - accuracy: 0.9375\n",
      "Epoch 00434: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0613 - accuracy: 0.9667 - val_loss: 0.0470 - val_accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0372 - accuracy: 1.0000\n",
      "Epoch 00435: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0610 - accuracy: 0.9667 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 00436: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0612 - accuracy: 0.9667 - val_loss: 0.0471 - val_accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0604 - accuracy: 0.9375\n",
      "Epoch 00437: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.9667 - val_loss: 0.0469 - val_accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 00438: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.9750 - val_loss: 0.0467 - val_accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1153 - accuracy: 0.9375\n",
      "Epoch 00439: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0612 - accuracy: 0.9750 - val_loss: 0.0463 - val_accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0292 - accuracy: 1.0000\n",
      "Epoch 00440: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0610 - accuracy: 0.9833 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0955 - accuracy: 0.9375\n",
      "Epoch 00441: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0608 - accuracy: 0.9667 - val_loss: 0.0468 - val_accuracy: 1.0000\n",
      "Epoch 442/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0505 - accuracy: 1.0000\n",
      "Epoch 00442: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9667 - val_loss: 0.0468 - val_accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1055 - accuracy: 0.8750\n",
      "Epoch 00443: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0606 - accuracy: 0.9667 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
      "Epoch 444/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 00444: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9667 - val_loss: 0.0464 - val_accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0909 - accuracy: 0.9375\n",
      "Epoch 00445: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9667 - val_loss: 0.0466 - val_accuracy: 1.0000\n",
      "Epoch 446/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0910 - accuracy: 1.0000\n",
      "Epoch 00446: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.9667 - val_loss: 0.0469 - val_accuracy: 1.0000\n",
      "Epoch 447/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 00447: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9583 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 448/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0491 - accuracy: 0.9375\n",
      "Epoch 00448: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9583 - val_loss: 0.0470 - val_accuracy: 0.9667\n",
      "Epoch 449/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1201 - accuracy: 0.9375\n",
      "Epoch 00449: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9667 - val_loss: 0.0466 - val_accuracy: 1.0000\n",
      "Epoch 450/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000\n",
      "Epoch 00450: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0607 - accuracy: 0.9667 - val_loss: 0.0465 - val_accuracy: 1.0000\n",
      "Epoch 451/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0371 - accuracy: 1.0000\n",
      "Epoch 00451: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.9750 - val_loss: 0.0467 - val_accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0565 - accuracy: 1.0000\n",
      "Epoch 00452: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0604 - accuracy: 0.9750 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
      "Epoch 453/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0723 - accuracy: 1.0000\n",
      "Epoch 00453: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0462 - val_accuracy: 1.0000\n",
      "Epoch 454/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 00454: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.9833 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0636 - accuracy: 1.0000\n",
      "Epoch 00455: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0600 - accuracy: 0.9750 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1089 - accuracy: 0.9375\n",
      "Epoch 00456: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0603 - accuracy: 0.9750 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0343 - accuracy: 1.0000\n",
      "Epoch 00457: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0598 - accuracy: 0.9833 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1047 - accuracy: 0.9375\n",
      "Epoch 00458: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.9750 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1117 - accuracy: 0.9375\n",
      "Epoch 00459: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.9750 - val_loss: 0.0460 - val_accuracy: 1.0000\n",
      "Epoch 460/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1112 - accuracy: 0.9375\n",
      "Epoch 00460: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0599 - accuracy: 0.9750 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
      "Epoch 461/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1303 - accuracy: 0.9375\n",
      "Epoch 00461: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9750 - val_loss: 0.0459 - val_accuracy: 1.0000\n",
      "Epoch 462/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1354 - accuracy: 0.9375\n",
      "Epoch 00462: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0597 - accuracy: 0.9750 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 00463: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0596 - accuracy: 0.9750 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
      "Epoch 464/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 00464: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9667 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0835 - accuracy: 1.0000\n",
      "Epoch 00465: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9667 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1480 - accuracy: 0.9375\n",
      "Epoch 00466: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9667 - val_loss: 0.0457 - val_accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0550 - accuracy: 1.0000\n",
      "Epoch 00467: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0594 - accuracy: 0.9667 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
      "Epoch 468/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0612 - accuracy: 0.9375\n",
      "Epoch 00468: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.9667 - val_loss: 0.0458 - val_accuracy: 1.0000\n",
      "Epoch 469/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0677 - accuracy: 1.0000\n",
      "Epoch 00469: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0595 - accuracy: 0.9750 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0929 - accuracy: 0.9375\n",
      "Epoch 00470: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0591 - accuracy: 0.9750 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
      "Epoch 471/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0559 - accuracy: 1.0000\n",
      "Epoch 00471: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0593 - accuracy: 0.9750 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
      "Epoch 472/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1020 - accuracy: 0.9375\n",
      "Epoch 00472: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.9750 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
      "Epoch 473/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0918 - accuracy: 0.9375\n",
      "Epoch 00473: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.9750 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1556 - accuracy: 0.8750\n",
      "Epoch 00474: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9750 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 00475: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0590 - accuracy: 0.9750 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 00476: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0592 - accuracy: 0.9750 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0293 - accuracy: 1.0000\n",
      "Epoch 00477: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9750 - val_loss: 0.0454 - val_accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0859 - accuracy: 1.0000\n",
      "Epoch 00478: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0222 - accuracy: 1.0000\n",
      "Epoch 00479: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0588 - accuracy: 0.9750 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0639 - accuracy: 1.0000\n",
      "Epoch 00480: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0455 - val_accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 00481: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0586 - accuracy: 0.9750 - val_loss: 0.0453 - val_accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1089 - accuracy: 0.9375\n",
      "Epoch 00482: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0450 - val_accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0178 - accuracy: 1.0000\n",
      "Epoch 00483: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0589 - accuracy: 0.9750 - val_loss: 0.0452 - val_accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1010 - accuracy: 0.9375\n",
      "Epoch 00484: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0584 - accuracy: 0.9750 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
      "Epoch 485/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 00485: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1090 - accuracy: 0.9375\n",
      "Epoch 00486: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0586 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1053 - accuracy: 1.0000\n",
      "Epoch 00487: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0873 - accuracy: 1.0000\n",
      "Epoch 00488: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0446 - val_accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0671 - accuracy: 1.0000\n",
      "Epoch 00489: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0562 - accuracy: 1.0000\n",
      "Epoch 00490: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9750 - val_loss: 0.0448 - val_accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 00491: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0587 - accuracy: 0.9750 - val_loss: 0.0449 - val_accuracy: 1.0000\n",
      "Epoch 492/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 00492: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0583 - accuracy: 0.9750 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0833 - accuracy: 1.0000\n",
      "Epoch 00493: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0176 - accuracy: 1.0000\n",
      "Epoch 00494: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0581 - accuracy: 0.9750 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 00495: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0584 - accuracy: 0.9833 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
      "Epoch 496/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 00496: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0440 - val_accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1696 - accuracy: 0.8750\n",
      "Epoch 00497: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.0438 - val_accuracy: 1.0000\n",
      "Epoch 498/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0543 - accuracy: 0.9375\n",
      "Epoch 00498: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0579 - accuracy: 0.9750 - val_loss: 0.0437 - val_accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.1710 - accuracy: 0.8750\n",
      "Epoch 00499: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0578 - accuracy: 0.9750 - val_loss: 0.0439 - val_accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0409 - accuracy: 1.0000\n",
      "Epoch 00500: val_accuracy did not improve from 1.00000\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0580 - accuracy: 0.9750 - val_loss: 0.0442 - val_accuracy: 1.0000\n",
      "Wall time: 19.7 s\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gráfico comparativo da Acurácia e Perda no treinamento"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "plot_history(history)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Apresentando a Acurácia final do Treino, carregando o melhor modelo salvo e apresentando a Acurácia do Teste"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "scores = model.evaluate(X_train, y_train)\r\n",
    "print()\r\n",
    "print(f\"Acuracia do Treino: {round(scores[1]*100,2)}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0577 - accuracy: 0.9750\n",
      "\n",
      "Acuracia do Treino: 97.5%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model = load_model(\"./modelo_mlp_ex3_1.hdf5\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "scores = model.evaluate(X_test, y_test)\r\n",
    "print()\r\n",
    "print(f\"Acuracia do Teste: {round(scores[1]*100,2)}%\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1543 - accuracy: 1.0000\n",
      "\n",
      "Acuracia do Teste: 100.0%\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c24c9dacf042e5cf8b743bae11b2cef3a95983df3bc5153773d9ffef1d5207d2"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf-gpu': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}